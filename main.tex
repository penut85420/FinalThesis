\documentclass{article}

\usepackage{CJKutf8}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{setspace}
\usepackage{wallpaper}
\usepackage[hyphens]{url}
\usepackage[margin=2.8cm]{geometry}
\addtolength{\wpXoffset}{+9.26cm}
\addtolength{\wpYoffset}{-12.5cm}

\usepackage{booktabs}
\usepackage{siunitx} % Required for alignment
\sisetup{
  round-mode = places, % Rounds numbers
  round-precision = 2, % to 2 places
}
\usepackage{listings}
\usepackage{color}

\title{Traditional Chinese Question Answering System}
\author{
  Wei-Ting Chen\\
  National Taiwan Ocean University\\
  \texttt{10757025@mail.ntou.edu.tw}\\
  \\
  Chuan-Jie Lin\\
  Nation Taiwan Ocean University\\
  \texttt{cjlin@mail.ntou.edu.tw}\\
}

\begin{document}

\maketitle
\pagenumbering{gobble}

\newpage

\CenterWallPaper{0.1}{ntou.png}
\doublespacing
\tableofcontents
\singlespacing

\newpage

\pagenumbering{arabic}
\section{Introduction}

\subsection{Motivation}
\paragraph{}
Here is the motivation of this research.

\subsection{Related Work}
\paragraph{}

\subparagraph{QA Dataset}
% CLQA, FGC

\subparagraph{MRC Dataset}
% SQuAD, KORSQuAD, DRCD

\subparagraph{QA Model}
% BiDAF, QANet, BERT, ALBERT, XLNet, ERINE, RoBERTa

\subsection{Thesis Architecture}
\paragraph{}
Here is the architecture of this thesis.

% [V] 解釋 QA, QA OPEN DOMAIN, QA MRC

% [ ] 提供資料集的範例
% [-] 資料集的目的，拿來測什麼的?
% [V] EX CLQA 是 OPEN domain DRCD 則是 MRC
\section{Datasets}
\paragraph{}
In order to train and evaluate a question answering model, it is necessary to use several datasets. There are two kinds of question answering datasets in this thesis - open domain and machine reading comprehension. An open domain question answering system can find an exact answer from a natural language question, while machine reading comprehension system can extract an answer from a specific article to a question.
\paragraph{}
To train a traditional Chinese question answering model, we need to use several question answering datasets such as NTCIR-CLQA, FGC, and DRCD. These datasets are used to test whether a model has ability to answer a question. Each of these dataset contains many articles, questions and answers. There are some different between these dataset, such as that NTCIR-CLQA is an open domain question answering dataset, while FGC and DRCD are machine reading comprehension datasets, but FGC has some more challenging questions which makes it not only a simple machine reading comprehension. There are more than one answers to each question in these dataset. Using these rich dataset, we are able to train a question answering model with good performance.

% 舉出資料中的範例作為表格呈現

\subsection{NTCIR-CLQA}
\paragraph{}
NTCIR-CLQA include NTCIR-5 CLQA\cite{sasaki2005ntcir5} and NTCIR-6\cite{sasaki2005ntcir6} CLQA. The articles of this dataset come from United Daily New, United Express, Min Sheng Daily and Economic Daily New between 1998 to 2005. We mixed two datasets and retrieved only traditional Chinese part of datasets. This dataset include 197 questions and 639 articles. Each question of CLQA may correspond to numerous articles, because these articles may contain the answer of the question. The subject of the article may has few relation to the question, which make this dataset quite challenging for current MRC models.

\paragraph{}
NTCIR-CLQA includes two parts - QA set and document set, both of files are represented in XML format.

\begin{CJK*}{UTF8}{bsmi}
\subparagraph{The QA set} is composed of many QA groups, and each QA group is composed of many questions and answers in different language. For each question and answer, there is an attribute indicating its language. For each answer, there is an attribute indicating its corresponding document ID. Note that answer may not be what it exactly showed in context, e.g. the answer provided by ground truth is "7年一億兩千萬美元", but the corresponding text showed in context is "七年一億兩千萬美元". There is also a field indicating the answer type of the QA group. The answer types are listed in table \ref{tab:answer-types}.
\end{CJK*}

\begin{table}[ht!]
  \centering
  \begin{tabular}{lr}
    \multicolumn{1}{c}{Answer Type} & \multicolumn{1}{c}{Proportion} \\
    \toprule
    ARTIFACT & 10.50\% \\
    DATE & 11.81\% \\
    LOCATION & 16.80\% \\
    MONEY & 6.04\% \\
    NUMEX & 7.61\% \\
    ORGANIZATION & 8.01\% \\
    PERCENT & 5.51\% \\
    PERSON & 30.45\% \\
    TIME & 3.28\%
  \end{tabular}
  \caption{Answer type and its proportion in NTCIR-CLQA.}
  \label{tab:answer-types}
\end{table}

\subparagraph{The document set} is composed of many news articles, each article has three fields - headline, date and paragraph. We merge these fields into one paragraph as the context in MRC dataset.

\paragraph{Preprocessing}
Due to lack of information of answer position, it won't be compatible with extractive model such as BERT. Therefore, we use BERT to generate possible position of 20-best predictions of NTCIR-CLQA, and take the best match one as the information of answer position.

\paragraph{}
After these steps, there are still some answer position that is not found. Then we modify the answer to the form that shown in the context, and using a statistical method to find out which answer span may be the most relevant to the question. First, we list all answer candidates that exactly match in context, and calculating the average of reciprocal of distance of each characters in question and candidate in context, if the character appears more than once, then choose the closest one; If the character never appear, then scored as 0. Finally, we take the answer span with highest score as the corresponding answer span for the question.

\paragraph{}
After clean up, there are 512 questions and 1,711 DQA pairs in total. Document set has 2,8114,471 documents but only 1,504 documents will be used.

% [ ] 每種題型個舉個例子
% [-] Basic Adv Exp 各有多少題 (數據待補)
% [V] 把 Train Test 分開來統計
% [ ] 數據待補
\subsection{FGC}
\paragraph{}
The FGC (Formosa Grand Challenge) QA dataset includes 1,271 questions with 150 paragraphs. These paragraphs come from Wikipedia, Wiki News, and news of Central News Agency. The questions of this dataset divided into three types: basic, advanced and application. Each answer of basic and advanced questions is a segment of the paragraph. The answer to an application question will be a long paragraph. There are 300, 300, and 20 questions in total of basic, advanced, and application respectively. We only use the basic and advanced questions of the dataset.

\subsection{DRCD}
\paragraph{}
The DRCD\cite{shao2018drcd} (Delta Reading Comprehension Dataset) is an open domain traditional Chinese MRC dataset. The dataset contains 2,108 articles from Wikipedia and 10,013 paragraphs with 33,928 questions in total. According to our statistics, the average length of paragraphs and questions is 437 and 21 characters.

\paragraph{}
DRCD is divided into three parts - training, dev and test, there are 26936, 3524, and 3493 questions respectively. The total number of characters is 6185. This dataset is available at GitHub\footnote{\label{drcd_github}https://github.com/DRCKnowledgeTeam/DRCD}.

% 為什麼需要 language Models
% 為什麼要用這三種
% 要串出實驗的因果 
% 猜 answer type 的方法也不一樣，包含是否題的分類跟CLQA的AT分類
\section{Methods}
\paragraph{}
In this thesis, we focus on how deep learning methods work on traditional Chinese QA datasets, such as recurrent neural network with attention flow and transformers.

\subsection{Language Models}
\paragraph{}
We use several language models as word embedding of recurrent neural network, such as Google NNLM, fastText\cite{bojanowski2017enriching} CBOW and Skip-Gram, and also self-trained word embedding.

\subparagraph{Google NNLM} is available from TensorflowHub\footnote{\url{https://tfhub.dev/s?module-type=text-embedding}}. There are two types of Chinese language model: 50-dimension and 128-dimension. Both of the models are trained on Chinese Google News with 100 billions corpus.

\subparagraph{fastText} is a method for efficient learning of word representation, training methods including CBOW (Continuous Bag of Words) and Skip-Gram. We retrieve text from Chinese wikidump and translate into traditional Chinese as training corpus, and use both CBOW and SG to generate 100, 200, 300 and 400 dimensions of word representation.

\subparagraph{Self-Trained Word Embedding} is that we trained word embedding as an embedding layer which is a part of whole QA model, all word embedding will be seen as trainable parameters of model. Due to its convenience, we can specify the dimension as any number.

% RNN 跟 TRS 算是同一種模組，要寫在同一章
% 畫圖是最後一個系統，指定某些層有沒有試過之類的
\subsection{Recurrent Neural Network}
\paragraph{}
We choose LSTM\cite{hochreiter1997lstm} (Long short-term memory) and GRU\cite{cho2014learning} (Gated recurrent unit) as our core of model architecture, both of them are variants of RNN. A LSTM unit is composed of a cell, an input gate, an output gate, and an forget gate. The cell remember the state of the current time-step, and three gates control the flow of the information. GRU is similar to LSTM but lack of output gate, which make GRU has fewer parameters and reduces the cost of model training. We also use the bidirectional version of both RNN layers.

\subsection{Transformer}
\paragraph{}
The structure of transformer\cite{vaswani2017attention} have been described in Vaswani et al., 2017, they use multi-head attention to build the encoder-decoder model, which use to solve the machine translation tasks. On the other hand, BERT\cite{devlin2018bert} (Devlin et al., 2019) also used the transformer architecture to achieved excellent performance on several natural language processing tasks, including SQuAD, an English machine reading comprehension dataset.

\paragraph{}
We use the official pre-trained model of Chinese of BERT\footnote{\url{https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip}} to fine-tune with our tasks using released code\footnote{\label{run_squad}https://github.com/google-research/bert}. This pre-trained model is trained with both simplified and traditional Chinese corpus, with about 110 million parameters.

\section{Experiments}
\paragraph{}

\subsection{Evaluation}
\paragraph{}
We use EM, F1, ACC, and MRR as our evaluation metrics. EM and F1 is almost as same as which mentioned in Rajpurkar et al. (2016)\cite{rajpurkar2016squad}. In our experiments, both metrics ignore punctuations, including full-width punctuations. In F1 score, the predictions and ground truth are compared at the character-level.

\paragraph{}
What's the different between EM and ACC is the total numbers of denominator. ACC regards the data of the same questions as a unit, and only the prediction with highest probability from results of models will be used. ACC measures the percentage of these predictions that match any one of the ground truth answers provided from dataset exactly. There is no different between ACC and EM for DRCD due to each of questions only appeared once.

\paragraph{}
MRR is the average reciprocal rank (1/$n$) of the highest rank $n$ of a correct answer for each question.

\subsection{Answer Type Classification}
\paragraph{}


\subsection{Baseline System}
\paragraph{}
We first reproduced the experiment of DRCD with BERT, training batch size is 8, learning rate is 3e-5,

\paragraph{}
Our performance is closed to the original paper. The EM/F1 of our experiment is 81.04\%/89.06\%. The original performance of EM/F1 is 82.34\%/89.59\%.

\subsection{CLQA}
\paragraph{}
We use BERT-DRCD to evaluate whole CLQA dataset as our baseline system. The EM, F1, ACC and MRR of the system is 73.174\%, 81.248\%, 74.423\%, and 82.918\%. Then we use K-fold cross-validation to split CLQA into 10 folds. The training data of each fold is used to fine-tune BERT and BERT-DRCD with different numbers of epochs. The results shown as table \ref{tab:bert-clqa}.

\begin{table}[ht!]
  \centering
  \begin{tabular}{cccccc}
    Base Model & CLQA Epochs & EM & F1 & ACC & MRR\\
    \toprule
    BERT-DRCDe2 & 0 & 73.174\% & 81.248\% & 74.423\% & 82.918\% \\
    BERT-DRCDe2 & 1 & 68.848\% & 77.280\% & 71.633\% & 80.616\% \\
    BERT-DRCDe2 & 2 & 71.711\% & 79.257\% & 72.794\% & 81.665\% \\
    BERT-DRCDe2 & 3 & 73.700\% & 81.498\% & 72.602\% & 82.153\% \\
    BERT-DRCDe2 & 4 & 71.304\% & 79.646\% & 72.583\% & 82.114\% \\
    BERT        & 1 & 26.769\% & 33.187\% & 26.625\% & 38.307\% \\
    BERT        & 2 & 49.441\% & 57.259\% & 51.686\% & 61.541\% \\
    BERT        & 3 & 52.128\% & 60.197\% & 51.674\% & 62.467\% \\
    BERT        & 4 & 55.990\% & 63.773\% & 55.980\% & 66.405\% \\
  \end{tabular}
  \caption{Evaluation results of CLQA with BERT. The model BERT-DRCDe2 means that BERT-Base-Chinese has been fine-tuned with DRCD for 2 epochs, while the model BERT means the BERT-Base-Chinese itself without any fine-tuning.}
  \label{tab:bert-clqa}
\end{table}

\paragraph{}
Due to the difficulty and insufficient numbers of training data of the CLQA dataset, the performance is quite bad when the BERT doesn't fine-tune with any thing, and the performance also drops at the first and second epochs of BERT-DRCDe2, but to the third epochs, the performance is closed to the BERT-DRCDe2 without CLQA fine-tuning.

\paragraph{}
Then we use BERT-DRCD to predict the CLQA, and split it into RIGHT and WRONG parts according to its predictions. Also, we split both part into 10 folds and use to fine-tune BERT-DRCD. The results of this experiment is as shown as table \ref{tab:bert-clqa-right-wrong}.

\begin{table}[ht!]
  \centering
  \caption{CLQA with BERT}
  \begin{tabular}{ccccc}
    CLQA Type & ACC & EM & F1\\
    \toprule
    RIGHT & 72.42\% & 76.22\% & 82.76\% \\
    WRONG & 30.48\% & 26.84\% & 36.60\% \\
  \end{tabular}
  \label{tab:bert-clqa-right-wrong}
\end{table}

\section{Conclusion}
\paragraph{}
Here is the conclusion.

\bibliography{main}
\bibliographystyle{ieeetr}

\end{document}
