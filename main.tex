\documentclass{article}

\usepackage{CJKutf8}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{setspace}
\usepackage{wallpaper}
\usepackage[hyphens]{url}
\usepackage[margin=2.8cm]{geometry}
\addtolength{\wpXoffset}{+9.26cm}
\addtolength{\wpYoffset}{-12.5cm}

\usepackage{booktabs}
\usepackage{siunitx} % Required for alignment
\sisetup{
  round-mode = places, % Rounds numbers
  round-precision = 2, % to 2 places
}
\usepackage{listings}
\usepackage{color}

\title{Traditional Chinese Question Answering System}
\author{
  Wei-Ting Chen\\
  National Taiwan Ocean University\\
  \texttt{10757025@mail.ntou.edu.tw}\\
  \\
  Chuan-Jie Lin\\
  Nation Taiwan Ocean University\\
  \texttt{cjlin@mail.ntou.edu.tw}\\
}

\begin{document}

\maketitle
\pagenumbering{gobble}

\newpage

\CenterWallPaper{0.1}{ntou.png}
\doublespacing
\tableofcontents
\singlespacing

\newpage

\pagenumbering{arabic}
\section{Introduction}

\subsection{Motivation}
\paragraph{}
Here is the motivation of this research.

\subsection{Related Work}
\paragraph{}

\subparagraph{QA Dataset}
% CLQA, FGC

\subparagraph{MRC Dataset}
% SQuAD, KORSQuAD, DRCD

\subparagraph{QA Model}
% BiDAF, QANet, BERT, ALBERT, XLNet, ERINE, RoBERTa

\subsection{Thesis Architecture}
\paragraph{}
Here is the architecture of this thesis.

% [V] 解釋 QA, QA OPEN DOMAIN, QA MRC

% [ ] 提供資料集的範例
% [-] 資料集的目的，拿來測什麼的?
% [V] EX CLQA 是 OPEN domain DRCD 則是 MRC
\section{Datasets}
\paragraph{}
In order to train and evaluate a question answering model, it is necessary to use several datasets. There are two kinds of question answering datasets in this thesis - open domain and machine reading comprehension. An open domain question answering system can find an exact answer from a natural language question, while the machine reading comprehension system can extract an answer from a specific article to a question.
\paragraph{}
To train a traditional Chinese question answering model, we need to use several questions answering datasets such as NTCIR-CLQA, FGC, and DRCD. These datasets are used to test whether a model has the ability to answer a question. Each of these datasets contains many articles, questions, and answers. There are some differences between these datasets, such as that NTCIR-CLQA is an open domain question answering dataset, while FGC and DRCD are machine reading comprehension datasets, but FGC has some more challenging questions which make it not only a simple machine reading comprehension. There is more than one answer to each question in these datasets. Using these rich datasets, we are able to train a question answering model with good performance.

% [ ] 舉例子

\subsection{NTCIR-CLQA}
\paragraph{}
NTCIR-CLQA includes NTCIR-5 CLQA\cite{sasaki2005ntcir5} and NTCIR-6\cite{sasaki2005ntcir6} CLQA. The articles of this dataset come from United Daily New, United Express, Min Sheng Daily, and Economic Daily New between 1998 to 2005. We mixed two datasets and retrieved only the traditional Chinese part of datasets. This dataset includes 197 questions and 639 articles. Each question of CLQA may correspond to numerous articles, because these articles may contain the answer to the question. The subject of the article may have few relations to the question, which makes this dataset quite challenging for current MRC models.

\paragraph{}
NTCIR-CLQA includes two parts - QA set and document set, both of the files are represented in XML format.

\begin{CJK*}{UTF8}{bsmi}
\subparagraph{The QA set} is composed of many QA groups, and each QA group is composed of many questions and answers in a different language. For each question and answer, there is an attribute indicating its language. For each answer, there is an attribute indicating its corresponding document ID. Note that answer may not be what it exactly showed in context, e.g. the answer provided by ground truth is "7年一億兩千萬美元", but the corresponding text showed in context is "七年一億兩千萬美元". An example is shown in table \ref{tab:clqa-qa}. There is also a field indicating the answer type of the QA group. The answer types are listed in table \ref{tab:answer-types}.
\end{CJK*}

\begin{CJK*}{UTF8}{bsmi}
  \begin{table}[ht!]
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
    Question & \multicolumn{3}{c|}{請問京都議定書規定幾個工業國家的二氧化碳排放量限制？} \\ \hline
    Answer & 37 & Support Document & edn\_xxx\_20001202\_0660071 \\ \hline
    Answer & 三十八 & Support Document & udn\_xxx\_20010330\_0832514 \\ \hline
    Answer Type & \multicolumn{3}{c|}{NUMEX} \\ \hline
    \end{tabular}
    \caption{An example of NTCIR-CLQA dataset. Note that answers might be different due to different support documents.}
    \label{tab:clqa-qa}
  \end{table}
\end{CJK*}

\begin{table}[ht!]
  \centering
  \begin{tabular}{lr}
    \multicolumn{1}{c}{Answer Type} & \multicolumn{1}{c}{Proportion} \\
    \toprule
    ARTIFACT & 10.50\% \\
    DATE & 11.81\% \\
    LOCATION & 16.80\% \\
    MONEY & 6.04\% \\
    NUMEX & 7.61\% \\
    ORGANIZATION & 8.01\% \\
    PERCENT & 5.51\% \\
    PERSON & 30.45\% \\
    TIME & 3.28\%
  \end{tabular}
  \caption{Answer type and its proportion in NTCIR-CLQA.}
  \label{tab:answer-types}
\end{table}

\subparagraph{The document set} is composed of many news articles, each article has three fields - headline, date, and paragraph. We merge these fields into one paragraph as the context in the MRC dataset.

\paragraph{Preprocessing}
Due to the lack of information on the answer position, it won't be compatible with the extractive model such as BERT. Therefore, we use BERT to generate the possible position of 20-best predictions of NTCIR-CLQA and take the best match one as the information of the answer position.

\paragraph{}
After these steps, there is still some answer position that is not found. Then we modify the answer to the form that shown in the context and using a statistical method to find out which answer span may be the most relevant to the question. First, we list all answer candidates that exactly match in context, and calculating the average of reciprocal of the distance of each character in question and candidate in context, if the character appears more than once, then choose the closest one; If the character never appears, then scored as 0. Finally, we take the answer span with the highest score as the corresponding answer span for the question.

\paragraph{}
After clean up, there are 512 questions and 1,711 DQA pairs in total. Document set has 2,8114,471 documents but only 1,504 documents will be used.

% [V] 每種題型個舉個例子
% [-] Basic Adv Exp 各有多少題 (數據待補)
% [ ] 把 Train Test 分開來統計
\subsection{FGC}
\paragraph{}
The FGC (Formosa Grand Challenge) QA dataset includes 1,271 questions with 150 paragraphs. These paragraphs come from Wikipedia, Wiki News, and news of Central News Agency. The questions of this dataset divided into three types: basic, advanced, and application. Most of the answers to basic and advanced questions are segments of the paragraph, except for some questions that asking for yes or no which may not be in the context. The answer to an application question will be a long paragraph. There are 300, 300, and 20 questions in a total of basic, advanced, and application respectively. We only use the basic and advanced questions of the dataset.

\begin{CJK*}{UTF8}{bsmi}
  \begin{table}[!ht]
    \centering
    \begin{tabular}{|c|c|l|}
      \hline
      \multicolumn{2}{|c|}{Article} & 蘇軾，眉州眉山（今四川省眉山市）人，北宋時…（略） \\ \hline
      \multirow{2}{*}{Basic} & Question & 蘇東坡在中國歷史上，是哪一個朝代的人？ \\ \cline{2-3} 
      & Answer & 北宋 \\ \hline
      \multirow{2}{*}{Advanced} & Question & 韓愈在中國歷史上，是哪一個朝代的人？ \\ \cline{2-3} 
      & Answer & 唐代 \\ \hline
      Application & Question & 蘇東坡為何被後人認為是文學藝術史上的通才？ \\ \hline
    \end{tabular}
  \end{table}
\end{CJK*}

\subsection{DRCD}
\paragraph{}
The DRCD\cite{shao2018drcd} (Delta Reading Comprehension Dataset) is an open domain traditional Chinese MRC dataset. The dataset contains 2,108 articles from Wikipedia and 10,013 paragraphs with 33,928 questions in total. According to our statistics, the average length of paragraphs and questions is 437 and 21 characters.

\paragraph{}
DRCD is divided into three parts - training, dev, and test, there are 26936, 3524, and 3493 questions respectively. The total number of characters is 6185. This dataset is available at GitHub\footnote{\label{drcd_github}https://github.com/DRCKnowledgeTeam/DRCD}.

% [ ] 為什麼需要 language Models
% [ ] 為什麼要用這三種
% [ ] 要串出實驗的因果 
% [ ] 猜 answer type 的方法也不一樣，包含是否題的分類跟 CLQA 的 AT 分類
\section{Methods}
\paragraph{}
In this thesis, we focus on how deep learning methods work on traditional Chinese QA datasets, such as the recurrent neural network with attention flow and transformers.

\subsection{Language Models}
\paragraph{}
We use several language models as word embedding of the recurrent neural network, such as Google NNLM, fastText\cite{bojanowski2017enriching} CBOW, and Skip-Gram, and also self-trained word embedding.

\subparagraph{Google NNLM} is available from TensorflowHub\footnote{\url{https://tfhub.dev/s?module-type=text-embedding}}. There are two types of Chinese language model: 50-dimension and 128-dimension. Both of the models are trained on Chinese Google News with 100 billion corpora.

\subparagraph{fastText} is a method for efficient learning of word representation, training methods including CBOW (Continuous Bag of Words) and Skip-Gram. We retrieve text from Chinese wikidump and translate into traditional Chinese as the training corpus, and use both CBOW and SG to generate 100, 200, 300, and 400 dimensions of the word representation.

\subparagraph{Self-Trained Word Embedding} is that we trained word embedding as an embedding layer which is a part of the whole QA model, all word embedding will be seen as trainable parameters of the model. Due to its convenience, we can specify the dimension as any number.

\subsection{Answer Types Classification}
\subsubsection{Yes-No Classification}
\subsubsection{Entity Types Classification}
% [V] RNN 跟 TRS 算是同一種模組，要寫在同一章
% [ ] 畫圖是最後一個系統，指定某些層有沒有試過之類的
\subsection{Answer Extraction}
\subsubsection{Method 1: Recurrent Neural Network}
\paragraph{}
We choose LSTM\cite{hochreiter1997lstm} (Long short-term memory) and GRU\cite{cho2014learning} (Gated recurrent unit) as our core of model architecture, both of them are variants of RNN. An LSTM unit is composed of a cell, an input gate, an output gate, and a forget gate. The cell remembers the state of the current time-step, and three gates control the flow of the information. GRU is similar to LSTM but lack of output gate, which make GRU has fewer parameters and reduces the cost of model training. We also use the bidirectional version of both RNN layers.

\subsubsection{Method 2: Transformer}
\paragraph{}
The structure of transformer\cite{vaswani2017attention} has been described in Vaswani et al., 2017, they use multi-head attention to build the encoder-decoder model, which uses to solve the machine translation tasks. On the other hand, BERT\cite{devlin2018bert} (Devlin et al., 2019) also used the transformer architecture to achieve excellent performance on several natural language processing tasks, including SQuAD, an English machine reading comprehension dataset.

\paragraph{}
We use the official pre-trained model of Chinese of BERT\footnote{\url{https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip}} to fine-tune with our tasks using released code\footnote{\label{run_squad}https://github.com/google-research/bert}. This pre-trained model is trained with both simplified and traditional Chinese corpus, with about 110 million parameters.

\section{Experiments}
\paragraph{}

\subsection{Evaluation}
\paragraph{}
We use EM, F1, ACC, and MRR as our evaluation metrics. EM and F1 are almost as same as mentioned in Rajpurkar et al. (2016)\cite{rajpurkar2016squad}. In our experiments, both metrics ignore punctuations, including full-width punctuations. In the F1 score, the predictions and ground truth are compared at the character-level.

\paragraph{}
What's the difference between EM and ACC is the total numbers of the denominator. ACC regards the data of the same questions as a unit, and only the prediction with the highest probability from the results of models will be used. ACC measures the percentage of these predictions that match any one of the ground truth answers provided from the dataset exactly. There is no difference between ACC and EM for DRCD due to each of the questions only appeared once.

\paragraph{}
MRR is the average reciprocal rank (1/$n$) of the highest rank $n$ of a correct answer for each question.

\subsection{Answer Type Classification}
\paragraph{}


\subsection{Baseline System}
\paragraph{}
We first reproduced the experiment of DRCD with BERT, training batch size is 8, and the learning rate is 3e-5,

\paragraph{}
Our performance is closed to the original paper. The EM/F1 of our experiment is 81.04\%/89.06\%. The original performance of EM/F1 is 82.34\%/89.59\%.

\subsection{CLQA}
\paragraph{}
We use BERT-DRCD to evaluate the whole CLQA dataset as our baseline system. The EM, F1, ACC, and MRR of the system are 73.174\%, 81.248\%, 74.423\%, and 82.918\%. Then we use K-fold cross-validation to split CLQA into 10 folds. The training data of each fold is used to fine-tune BERT and BERT-DRCD with different numbers of epochs. The results shown as table \ref{tab:bert-clqa}.

\begin{table}[ht!]
  \centering
  \begin{tabular}{cccccc}
    Base Model & CLQA Epochs & EM & F1 & ACC & MRR\\
    \toprule
    BERT-DRCDe2 & 0 & 73.174\% & 81.248\% & 74.423\% & 82.918\% \\
    BERT-DRCDe2 & 1 & 68.848\% & 77.280\% & 71.633\% & 80.616\% \\
    BERT-DRCDe2 & 2 & 71.711\% & 79.257\% & 72.794\% & 81.665\% \\
    BERT-DRCDe2 & 3 & 73.700\% & 81.498\% & 72.602\% & 82.153\% \\
    BERT-DRCDe2 & 4 & 71.304\% & 79.646\% & 72.583\% & 82.114\% \\
    BERT        & 1 & 26.769\% & 33.187\% & 26.625\% & 38.307\% \\
    BERT        & 2 & 49.441\% & 57.259\% & 51.686\% & 61.541\% \\
    BERT        & 3 & 52.128\% & 60.197\% & 51.674\% & 62.467\% \\
    BERT        & 4 & 55.990\% & 63.773\% & 55.980\% & 66.405\% \\
  \end{tabular}
  \caption{Evaluation results of CLQA with BERT. The model BERT-DRCDe2 means that BERT-Base-Chinese has been fine-tuned with DRCD for 2 epochs, while the model BERT means the BERT-Base-Chinese itself without any fine-tuning.}
  \label{tab:bert-clqa}
\end{table}

\paragraph{}
Due to the difficulty and insufficient numbers of training data of the CLQA dataset, the performance is quite bad when the BERT doesn't fine-tune with anything, and the performance also drops at the first and second epochs of BERT-DRCDe2, but to the third epochs, the performance is closed to the BERT-DRCDe2 without CLQA fine-tuning.

\paragraph{}
Then we use BERT-DRCD to predict the CLQA and split it into RIGHT and WRONG parts according to its predictions. Also, we split both parts into 10 folds and use to fine-tune BERT-DRCD. The results of this experiment is as shown as table \ref{tab:bert-clqa-right-wrong}.

\begin{table}[ht!]
  \centering
  \caption{CLQA with BERT}
  \begin{tabular}{ccccc}
    CLQA Type & ACC & EM & F1\\
    \toprule
    RIGHT & 72.42\% & 76.22\% & 82.76\% \\
    WRONG & 30.48\% & 26.84\% & 36.60\% \\
  \end{tabular}
  \label{tab:bert-clqa-right-wrong}
\end{table}

\section{Conclusion}
\paragraph{}
Here is the conclusion.

\bibliography{main}
\bibliographystyle{ieeetr}

\end{document}
