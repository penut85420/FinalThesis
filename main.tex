\documentclass{article}

\usepackage{multirow}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{setspace}
\usepackage{wallpaper}
\usepackage[margin=2.8cm]{geometry}
\addtolength{\wpXoffset}{+9.26cm}
\addtolength{\wpYoffset}{-12.5cm}

\usepackage{booktabs}
\usepackage{siunitx} % Required for alignment
\sisetup{
  round-mode = places, % Rounds numbers
  round-precision = 2, % to 2 places
}
\usepackage{listings}
\usepackage{color}

\title{Traditional Chinese Question Answering System}
\author{
  Wei-Ting Chen\\
  National Taiwan Ocean University\\
  \texttt{10757025@mail.ntou.edu.tw}\\
  \\
  Chuan-Jie Lin\\
  Nation Taiwan Ocean University\\
  \texttt{cjlin@mail.ntou.edu.tw}\\
}

\begin{document}

\maketitle
\pagenumbering{gobble}

\newpage

\CenterWallPaper{0.1}{ntou.png}
\doublespacing
\tableofcontents
\singlespacing

\newpage

\pagenumbering{arabic}
\section{Introduction}

\subsection{Motivation}
\paragraph{}
Here is the motivation of this research.

\subsection{Related Work}
\paragraph{}
Here are related works.

\subsection{Thesis Architecture}
\paragraph{}
Here is the architecture of this thesis.

\section{Datasets}
\paragraph{}
We use several traditional Chinese QA datasets such as NTCIR-CLQA, FGC, and DRCD. Each of these dataset contains many articles, questions and answers. All of these dataset have more than one answer to each question.

\subsection{NTCIR-CLQA}
\paragraph{}
NTCIR-CLQA include NTCIR-5 CLQA\cite{sasaki2005ntcir5} and NTCIR-6\cite{sasaki2005ntcir6} CLQA. The articles of this dataset come from United Daily New, United Express, Min Sheng Daily and Economic Daily New between 1998 to 2005. We mixed two datasets and retrieved only traditional Chinese part of datasets. This dataset include 197 questions and 639 articles. Each question of CLQA may correspond to numerous articles, because these articles may contain the answer of the question. The subject of the article may has few relation to the question, which make this dataset quite challenging for current MRC models.

\paragraph{}
NTCIR-CLQA includes two parts - QA set and document set, both of files are represented in XML format.

\subparagraph{The QA set} is composed of many QA groups, and each QA group is composed of many questions and answers in different language. For each question and answer, there is an attribute indicating its language. For each answer, there is an attribute indicating its corresponding document ID. There is also a field indicating the answer type of the QA group. The answer types are listed in table \ref{tab:answer-types}.

\begin{table}[h!]
  \centering
  \begin{tabular}{lr}
    \multicolumn{1}{c}{Answer Type} & \multicolumn{1}{c}{Proportion} \\
    \toprule
    ARTIFACT & 10.50\% \\
    DATE & 11.81\% \\
    LOCATION & 16.80\% \\
    MONEY & 6.04\% \\
    NUMEX & 7.61\% \\
    ORGANIZATION & 8.01\% \\
    PERCENT & 5.51\% \\
    PERSON & 30.45\% \\
    TIME & 3.28\%
  \end{tabular}
  \caption{Answer type and its proportion in NTCIR-CLQA QA-set.}
  \label{tab:answer-types}
\end{table}

\subparagraph{The document set} is composed of many news articles, each article has three fields - headline, date and paragraph. We merge these fields into one paragraph as the context in MRC dataset.

\paragraph{Preprocessing}
Due to lack of information of answer position, it won't be compatible with extractive model such as BERT. Therefore, we use BERT to generate possible position of 20-best predictions of NTCIR-CLQA, and take the best match one as the information of answer position.

% 還有剩餘約三成的 DQA Pairs 我們使用計算 Keyword 距離的方式來尋找最佳符合的答案位置

After clean up, there are 512 questions and 1,711 DQA pairs in total. Document set has 2,8114,471 documents but only 1,504 documents will be used.

\subsection{FGC}
\paragraph{}
The FGC (Formosa Grand Challenge) QA dataset includes 1,271 questions with 150 paragraphs. These paragraphs come from Wikipedia, Wiki News, and news of Central News Agency. The questions of this dataset divided into three types: basic, advanced and application. Each answer of basic and advanced questions is a segment of the paragraph. We only use the basic and advanced questions of the dataset.

\subsection{DRCD}
\paragraph{}
The DRCD\cite{shao2018drcd} (Delta Reading Comprehension Dataset) is an open domain traditional Chinese MRC dataset. The dataset contains 2,108 articles from Wikipedia and 10,013 paragraphs with 33,928 questions in total. According to our statistics, the average length of paragraphs and questions is 437 and 21 characters.

\section{Methods}
\paragraph{}
In this paper, we focus on how deep learning methods work on traditional Chinese QA datasets, such as recurrent neural network and transformers.

\subsection{Recurrent Neural Network}
\paragraph{}
We choose LSTM\cite{hochreiter1997lstm} (Long short-term memory) and GRU\cite{cho2014learning} (Gated recurrent unit) as our core of model architecture, both of them are variants of RNN. A LSTM unit is composed of a cell, an input gate, an output gate, and an forget gate. The cell remember the state of the current time-step, and three gates control the flow of the information. GRU is similar to LSTM but lack of output gate, which make GRU has fewer parameters and reduces the cost of model training. We also use the bidirectional version of both RNN layers.

\subsection{Transformer}
\paragraph{}
The structure of transformer\cite{vaswani2017attention} have been described in Vaswani et al., 2017, they use multi-head attention to build the encoder-decoder model, which use to solve the machine translation tasks. On the other hand, BERT\cite{devlin2018bert} (Devlin et al., 2019) also used the transformer architecture to achieved excellent performance on several natural language processing tasks, including SQuAD, an English machine reading comprehension dataset.

\paragraph{}
We use the official pretrained model of Chinese to fine-tune with our tasks. This pretrained model is trained with both simplified and traditional Chinese corpus, with about 110 million parameters.

\section{Experiments}
\paragraph{}

\subsection{Evaluation}
\paragraph{}
We use ACC, EM, and F1 as our evaluation metrics. EM and F1 is as same as which mentioned in SQuAD\cite{rajpurkar2016squad} (Rajpurkar et al., 2016). What's the different between EM and ACC is the total numbers of denominator. ACC regards the data of the same questions as a unit, and only the prediction with highest probability from results of models will be used. ACC measures the percentage of these predictions that match any one of the ground truth answers provided from dataset exactly. There is no different between ACC and EM for DRCD due to each of questions only appeared once.

\subsection{Baseline System}
\paragraph{}
We first reproduced the experiment of DRCD with BERT, our performance is closed to the original paper. The EM and F1 of our experiment is 81.04\% and 89.06\%. The original performance of EM and F1 is 82.34\% and 89.59\%.

\subsection{CLQA}
\paragraph{}
We use BERT-DRCD to evaluate whole CLQA dataset as our baseline system. The ACC, EM, and F1 of the system is 74.62\%, 69.36\%, and 79.37\% . Then we use K-fold cross-validation to split CLQA into 10 folds. We use training data of each fold to fine-tune BERT and BERT-DRCD with different numbers of epochs, the results shown as table \ref{tab:bert-clqa}.

\begin{table}[h!]
  \centering
  \caption{CLQA with BERT}
  \begin{tabular}{ccccc}
    Base Model & CLQA Epochs & ACC & EM & F1\\
    \toprule
    BERT-DRCD & 2 & 61.78\% & 64.48\% & 71.86\% \\
    BERT-DRCD & 1 & 64.97\% & 63.39\% & 71.39\% \\
    BERT-DRCD & 0 & 74.62\% & 69.36\% & 79.37\% \\
    BERT      & 2 & 29.93\% & 28.18\% & 33.17\% \\
  \end{tabular}
  \label{tab:bert-clqa}
\end{table}

\paragraph{}
Then we use BERT-DRCD to predict the CLQA, and split it into RIGHT and WRONG parts according to its predictions. Also, we split both part into 10 folds and use to fine-tune BERT-DRCD. The results of this experiment is as shown as table \ref{tab:bert-clqa-right-wrong}.

\begin{table}[h!]
  \centering
  \caption{CLQA with BERT}
  \begin{tabular}{ccccc}
    CLQA Type & ACC & EM & F1\\
    \toprule
    RIGHT & 72.42\% & 76.22\% & 82.76\% \\
    WRONG & 30.48\% & 26.84\% & 36.60\% \\
  \end{tabular}
  \label{tab:bert-clqa-right-wrong}
\end{table}

\section{Conclusion}
\paragraph{}
Here is the conclusion.

\bibliography{main}
\bibliographystyle{ieeetr}

\end{document}
