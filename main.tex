% !TeX encoding = UTF-8
\documentclass{article}

\usepackage{CJKutf8}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{setspace}
\usepackage{wallpaper}
\usepackage{adjustbox}
\usepackage{afterpage}
\usepackage{lscape}
\usepackage{mathptmx}
\usepackage{titlesec}
\usepackage[hyphens]{url}
\usepackage[margin=2.8cm]{geometry}
\addtolength{\wpXoffset}{+9.26cm}
\addtolength{\wpYoffset}{-12.5cm}
\usepackage[backend=bibtex]{biblatex}

\bibliography{main}

\usepackage{booktabs}
\usepackage{siunitx} % Required for alignment
\sisetup{
  round-mode = places, % Rounds numbers
  round-precision = 2, % to 2 places
}
\usepackage{listings}
\usepackage{color}

\renewcommand\lstlistingname{Figure}
\renewcommand\lstlistlistingname{Figures}

\DeclareCaptionLabelFormat{tablel}{#1 #2}
\captionsetup[table]{labelsep=period}
\captionsetup[figure]{labelsep=period}

\lstset{
  language=XML,
  morekeywords={encoding,
  xs:schema,xs:element,xs:complexType,xs:sequence,xs:attribute}
  }
  % \setlist[description]{leftmargin=\parindent,labelindent=\parindent}

  % 以深度學習辨識中文文本蘊涵關係

  \title{Recognizing Chinese Textual Entailment by Deep Learning}
  \author{
    Wei-Ting Chen\\
    National Taiwan Ocean University\\
    \texttt{10757025@mail.ntou.edu.tw}\\
    \\
    Chuan-Jie Lin\\
    Nation Taiwan Ocean University\\
    \texttt{cjlin@mail.ntou.edu.tw}\\
    }

    \begin{document}
    \begin{CJK*}{UTF8}{bkai}

      \begin{titlepage}
  \centering
  {\Huge 國立臺灣海洋大學\par}
  \vspace{1cm}
  {\huge 資訊工程學系\par}
  \vspace{1cm}
  {\huge 碩士學位論文\par}
  \vspace{2cm}
  {\LARGE 指導教授：林川傑\ (Lin, Chuan-Jie) 博士\par}
  \vfill
  {\LARGE 以深度學習辨識中文文本蘊涵關係\par}
  {\LARGE Recognizing Chinese Textual Entailment by Deep Learning\par}
  \vfill
  {\LARGE 研究生：陳威廷\ \bigg(
    \begin{tabular}{c}
      Chen, Wei-Ting \\
      M00000000
    \end{tabular}
    \bigg) 撰}
    \vfill
    {\LARGE 中華民國\ 110 年\ 03 月}
  \end{titlepage}

  \newpage

  \begin{titlepage}
    \centering
  {\LARGE 以深度學習辨識中文文本蘊涵關係\par}
  \vfill
  {\LARGE Recognizing Chinese Textual Entailment by Deep Learning\par}
  \vfill
  {
    \LARGE
    \begin{minipage}{2in}
      研究生：陳威廷 \\
      指導教授：林川傑
    \end{minipage}
    \hfill
    \begin{minipage}{2.4in}
      Student: Chen, Wei-Ting \\
      Advisor: Lin, Chuan-Jie
    \end{minipage}
    \par
    }
    \vfill
    {\LARGE 國立臺灣海洋大學\par}
    {\LARGE 資訊工程學系\par}
    {\LARGE 碩士學位論文\par}
    \vfill
    {
      \LARGE
      A Thesis \\
      Submitted to Computer Science and Engineering \\
      Electrical Engineering and Computer Science \\
      National Taiwan Ocean University \\
      In Partial Fulfillment of the Requirements \\
      for the Degree of \\
      Master of Science \\
      in \\
      Computer Science and Engineering \\
      March, 2021 \\ \vspace{0.5cm}
      Keelung, Taiwan, Republic of China
      }
      \vfill
      {\LARGE 中華民國\ 110 年\ 03 月}
\end{titlepage}

\pagenumbering{gobble}

\newpage

\CenterWallPaper{0.1}{ntou.png}
\doublespacing
\tableofcontents
\singlespacing

\newpage

\listoffigures
\listoftables

\newpage

\pagenumbering{arabic}

\titleformat{\section}{\newpage\centering\normalfont\huge}{Chapter \thesection.}{1em}{}
\section{Introduction}

\subsection{Motivation}
\paragraph{}
% NLI 任務在定義上並不複雜，給定一句 Premise 和一句 Hypothesis，並辨識這兩句話是否存在蘊含關係。一個文意蘊含任務的分類有很多種類型，最簡單的分法就是「有蘊含」與「沒有蘊含」，而「沒有蘊含」的部份可以進一步的分成「互斥」與「獨立」，「有蘊含」的部份也可以進一步的分成「正向蘊含」與「雙向蘊含」。去判別兩句話之間是否存在蘊含關係，這個任務的難度會因為涉及的語言現象而產生很高的複雜度
The definition of the textual entailment (also called natural language inference) task is not complex, given a premise sentence and a hypothesis sentence, and recognizing if the two sentences are entailment. The category of the textual entailment tasks has many types, the simplest one is to divide into ``entailment'' and ``not entailment''. Further, ``not entailment'' can be divided into ``contradiction'' and ``independence'', and ``entailment'' can be divided into ``forward entailment'' and ``bi-directional entailment''. The determination of the relation of the two sentences will become highly difficult according to the complexity of the linguistic phenomenon that the sentences involved.

\paragraph{}
% 模型是否能夠精準判別此蘊含關係，是衡量此模型的自然語言理解能力很重要的指標。在許多自然語言的應用中，例如：問答系統、文件摘要、資訊擷取與機器翻譯等，若是能提高模型的語言理解能力，將會是相當有幫助的事情。
Whether a model can accurately inference the relation of the sentences is quite important indicator to measure the natural language understanding ability of the model. Increasing the NLU ability of the model will be helpful in many applications of the NLP, for example, question-answering system, summarization, information extraction, and machine translation.

\subsection{Related Work}
\paragraph{}
In English NLI datasets, the Stanford Natural Language Inference (SNLI)\cite{snli:emnlp2015} is a well-known corpus of the NLI datasets, which is a collection of 570k human-written English sentence pairs. GLUE benchmark collected some datasets as their NLT task benchmark, including RTE, MNLI, QNLI, WNLI. The Recognizing Textual Entailment (RTE) datasets come from a series of annual textual entailment challenges. RTE1\cite{dagan2006pascal}, RTE2\cite{bar2006second}, and RTE3\cite{giampiccolo2007third} are provided from PASCAL\footnote{http://pascallin.ecs.soton.ac.uk/}, RTE4, RTE5\cite{bentivogli2009fifth}, RTE6, and RTE7 are provided from NIST\footnote{https://www.nist.gov/}. The Multi-Genre Natural Language Inference (MultiNLI, or MNLI)\cite{N18-1101} is a crowd-sourced collection of 433k sentence pairs collected from a wide range of genres. Question-answering NLI (QNLI)\cite{wang2019glue} dataset is an NLI dataset converting from the Stanford Question Answering Dataset v1.1 (SQuAD). The Winograd NLI dataset is also an augmented dataset, which is come from a reading comprehension task called The Winograd Schema Challenge.

\paragraph{}
Chinese Natural Language Inference (CNLI)\footnote{https://github.com/blcunlp/CNLI} is a Simplified Chinese dataset from a sub task of The Seventeenth China National Conference on Computational Linguistics (CCL 2018) which has 90k sentence pairs. Original Chinese Natural Language Inference (OCNLI) is a NLI dataset with 50k sentence pairs, and it is a part of CLUE benchmark\footnote{https://www.cluebenchmarks.com/}. In NTCIR-9\cite{ntcir9rite1}, NTCIR-10\cite{ntcir10rite2}, and NTCIR-11\cite{ntcir11rite-val}, they proposed the RITE datasets, which have Japanese, Traditional Chinese, and Simplified Chinese task.
% Related NLI dataset, RTE\cite{dagan2006pascal}\cite{bar2006second}\cite{giampiccolo2007third}, SNLI, MNLI, QNLI\cite{wang2019glue}, CNLI, OCNLI, RITE.

% Related Methods 對這方面的 survey 較少，需要更多參考。
\paragraph{}
Zanzotto et al. (2006)\cite{zanzotto_moschitti_2006} conduct the experiments with the datasets of the RTE 2005 challenge by using cross-pair similarities. Wang et al. (2007) have achieved an accuracy of 66.9\% on the RTE-3 dataset by using sentence similarity based on dependency tree skeletons. Dinu et al. (2009)\cite{dinu_wang_2009} focused on the inference rules and try to find the improvement. Su et al.\cite{su_zheng_2011} achieved the best performance on NTCIR-9 RITE by using C4.5 decision tree base on the lexical and semantic features. Wang et al. (2013)\cite{wang-etal-2013-labeled} achieved the best performance on NTCIR-10 RITE2 (Simplified Chinese) by labeled alignment.

% 與我們同實驗室的 Tu 等人在 RITE1 與 RITE2 的 BC Task 上取得了 71.05% 與 66.62% 的 F-scores，而在 MC Task 上則達到 53.27% 與 48.39% 的 F-scores. 這些成果都比 NTCIR-9 與 NTCIR-10 的參賽系統都來得好。之後的 Liu 等人則擴展了上個方法，將之應用在 RITE-VAL 上，其效能也同樣勝過 NTCIR-11 的參賽系統。
Tu et al. (2015)\cite{tu_2015} using the rule-based classifier and combine with the SVM classifier, reached the F-scores of 71.05\% and 66.62\% to the BC task of RITE1 and RITE2, and the F-scores of 53.27\% and 48.39\% to the MC task, all of them are better than the best system of NTCIR-9 and NTCIR-10. Liu et al. (2016)\cite{liu_2016} expanded the methods and applied them on RITE-VAL, which has a better performance compared with the best system of NTCIR-11.

% BERT, RoBERTa, ALBERT, SemBERT, MTDNN, XLNet

\subsection{Thesis Architecture}
\paragraph{}
% 本論文的章節編排如下：我們首先會在第二章介紹與分析實驗所需使用到的資料集。在第三章會解說實驗會用到的方法與這些方法所需用到的資源。實驗結果與分析將會放在第四章。而第五章會總結本論文的研究成果與未來可以改進的方向。
The chapters of this thesis are organized as follows: We will introduce and analyze the datasets we used in chapter 2. In chapter 3, we will explain the methods and the resources that using in the methods. The results of the experiments will be discussed in chapter 4. Finally, we will conclude in chapter 5 and propose the direction of improvement.

\section{Datasets}
\paragraph{}
% NLI 資料集是數組句對的集合，每組句對由一個「前提句」與一個「假設句」所組成，這兩句話會形成一個推論，用來表示兩句話之間的關係，這樣的任務被稱為 NLI / RTE。而本論文主要研究的對象 - RITE 資料集則是使用了雙向、單向、互斥與獨立四個關係。
An NLI dataset is a collection of sentence pairs, each sentence pairs, premise and hypothesis, has an inference, which indicates the relation of two sentences, this kind of task is known as natural language inference (NLI), also known as recognizing textual entailment (RTE). In this section, we will introduce the datasets that we use in our experiments.

\paragraph{}
% 常見的 NLI 資料集例如 SNLI, MNLI 與 CNLI 等，通常使用了蘊含、互斥與獨立三個標籤，我們稱之為 ECN Task。這些標籤的含意如下：
The relations of common NLI datasets, for example SNLI\cite{snli:emnlp2015}, MNLI\cite{N18-1101}, CNLI\footnote{https://github.com/blcunlp/CNLI}, and OCNLI\cite{ocnli}, usually represent in three types of labels: entailment, contradiction, and neural, we called these dataset as ECN task. The meaning of these labels are as following:

\subparagraph{entailment} means that the premise entails the hypothesis, and whether the hypothesis entails the premise does not matter.
\begin{table}[ht!]
  \centering
  \begin{tabular}{|c|l|}
    \hline
    Premise & The way we try to approach it is to identify every legal problem that a client has. \\ \hline
    Hypothesis & All of the client's legal problems are supposed to be identified. \\ \hline
  \end{tabular}
  \caption[An example of an entailment pair from MNLI]{An example of an entailment pair from MNLI. Both of the sentences are talking about identifying the legal problems of the client, so this pair is marked as entailment.}
\end{table}

\subparagraph{contradiction} means that the premise and the hypothesis cannot be true at the same time.

\begin{table}[ht!]
  \centering
  \begin{tabular}{|c|l|}
    \hline
    Premise & There was food for all, and houses had been conjured hastily to shelter the people. \\ \hline
    Hypothesis & There was not enough food for all sadly. \\ \hline
  \end{tabular}
  \caption[An example of a contradiction pair from MNLI]{An example of a contradiction pair from MNLI. The premise says that the food is enough but the hypothesis says it is not, so this pair is marked as contradiction.}
\end{table}

\subparagraph{neural} means that the premise does not have any relation mentioned above with the hypothesis.

\begin{table}[ht!]
  \centering
  \begin{tabular}{|c|l|}
    \hline
    Premise & Well i think that's about all my pet stories right now so. \\ \hline
    Hypothesis & My pets are up to many antics, and I'm happy I got to share these. \\ \hline
  \end{tabular}
  \caption[An example of a neutral pair from MNLI]{An example of a neutral pair from MNLI. The premise is talking about the story of the pet, but the hypothesis is talking about the antics of the pet, they are irrelevant to each other, so this pair is marked as neutral.}
\end{table}

\paragraph{}
And in this thesis, our main target datasets - RITE, have four types of labels: B (bi-directional entailment), F (forward entailment), C (contradiction), and I (independence), we called these dataset as BFCI task.

\subparagraph{bi-directional entailment} means that the premise entails the hypothesis, and the hypothesis entails the premise.

\begin{table}[ht!]
  \centering
  \begin{tabular}{|c|l|}
    \hline
    Premise & 約瑟夫·傅立葉是十九世紀法國數學家及物理學家。 \\ \hline
    Hypothesis & 十九世紀法國的約瑟夫·傅立葉是數學家、物理學家。 \\ \hline
  \end{tabular}
  \caption[An example of a bi-directional entailment pair from RITE-VAL]{An example of a bi-directional entailment pair from RITE-VAL. The hypothesis is a paraphrase of the premise, they are talking about the same thing, so this pair is marked as bi-directional entailment.}
\end{table}

\subparagraph{forward entailment} means that the premise entails the hypothesis, but the hypothesis does not entail the premise.

\begin{table}[ht!]
  \centering
  \begin{tabular}{|c|l|}
    \hline
    Premise & 約瑟夫·傅立葉是十九世紀法國數學家、物理學家。 \\ \hline
    Hypothesis & 約瑟夫·傅立葉是物理學家。 \\ \hline
  \end{tabular}
  \caption[An example of a forward entailment pair from RITE-VAL]{An example of a forward entailment pair from RITE-VAL. The premise says that Joseph Fourier is a person of 19 century, but the hypothesis does not mention the 19 century, so the premise can entail the hypothesis but the hypothesis can not entail the premise, this pair is marked as forward entailment.}
\end{table}

\subparagraph{contradiction} as same as the meaning in the ECN task.

\subparagraph{independence} as same as the meaning of neural in the ECN task.

\paragraph{}
In addition to the BFCI task and the BC task, both of them can be converted to BC task which is short for the binary classification task. The label of ``entailment'', ``bi-directional entailment'', and ``forward entailment'' will be marked as ``true'', and the label of ``contradiction'', ``neural'', and ``independence'' will be marked as ``false''.

\subsection{RITE}
\paragraph{}
RITE (Recognizing Inference in TExt) is the sub task in NTCIR conference, including Traditional Chinese, Simplified Chinese, and Japanese. In this research, we use the Traditional Chinese of RITE2 and RITE-VAL as our major evaluation datasets, which are come from NTCIR-10\cite{ntcir10rite2} and NTCIR-11\cite{ntcir11rite-val} these two conferences separately. They are collected from variety topics, such as domestic, history, politics, medicine and economy.

% 在 NTCIR-9 的時候同樣有舉辦 RITE 的 Task，其資料集為 RITE1，然而他的 dev set 與 test set 都被放入 RITE2 的 dev set 裡面，所以本研究只有使用 RITE2。
\paragraph{}
There is a RITE task in NTCIR-9 too, and the dataset it used is called RITE1\cite{ntcir9rite1}. However, the training set and test set of RITE1 had been merged into the training set of RITE2, so we only use RITE2 in this research.

\paragraph{}
% RITE-VAL 與 RITE2 的格式基本上相同，他們都有前提句 t1 與假設句 t2 以及他們的 id 與 label，而 RITE-VAL 則額外提供了「種類」的資訊，用來表示他們的句對其推論關係所涉及到的語言現象。
The formatting of RITE-VAL and RITE2 are the same, they both have the premise $t_1$, the hypothesis $t_2$, their pair id, and the label, while RITE-VAL provides the information of "category" to indicate the linguistic phenomenon of the relation of the sentence pair. RITE-VAL also includes the reversed labels in the training set. There are 28 linguistic phenomena are defined in RITE-VAL and listed in table \ref{tab:linguistic_phenomenon}. We only use this information to split RITE-VAL into 10 folds, and it is not a critical role in our experiments, so we didn't give a detailed description of the phenomenon.
% phenomenon - singular, phenomena - plural
\begin{table}[ht!]
  \centering
  \subfloat[The linguistic phenomenon related to entailment]{
    \begin{tabular}{|l|r|r|}
      \hline
      Category & Training & Test \\ \hline
      abbreviation & 6 & 25 \\ \hline
      apposition & 7 & 25 \\ \hline
      case\_alternation & 21 & 27 \\ \hline
      clause & 25 & 59 \\ \hline
      coreference & 11 & 24 \\ \hline
      hypernymy & 30 & 27 \\ \hline
      inference & 75 & 184 \\ \hline
      lexical\_entailment & 12 & 29 \\ \hline
      list & 20 & 37 \\ \hline
      meronymy & 4 & 23 \\ \hline
      modifier & 37 & 131 \\ \hline
      paraphrase & 47 & 49 \\ \hline
      quantity & 11 & 29 \\ \hline
      relative\_clause & 6 & 36 \\ \hline
      scrambling & 27 & 35 \\ \hline
      spatial & 18 & 42 \\ \hline
      synonymy:lex & 48 & 51 \\ \hline
      temporal & 11 & 40 \\ \hline
      transparent\_head & 13 & 26 \\ \hline
    \end{tabular}
  }
  \quad
  \subfloat[The linguistic phenomenon related to contradiction]{
    \begin{tabular}{|l|r|r|}
      \hline
      Category & Training & Test \\ \hline
      antonym & 20 & 35 \\ \hline
      exclusion:common\_sense & 8 & 34 \\ \hline
      exclusion:modality & 12 & 38 \\ \hline
      exclusion:modifier & 14 & 33 \\ \hline
      exclusion:predicate\_argument & 51 & 38 \\ \hline
      exclusion:quantity & 6 & 29 \\ \hline
      exclusion:spatial & 14 & 32 \\ \hline
      exclusion:temporal & 7 & 34 \\ \hline
      negation & 20 & 28 \\ \hline
    \end{tabular}
  }
  \caption{The linguistic phenomena distribution in RITE-VAL.}
  \label{tab:linguistic_phenomenon}
\end{table}

\paragraph{}
% RITE2 的 dev set 有 1321 組句對，test set 有 881 組句對，RITE-VAL 的 dev set 有 581 組句對，test set 有 1200 組句對。雖然 RITE2 資料量較多，但他們都是小規模的資料集。
RITE2 has 1,321 sentence pairs in the training set and 881 sentence pairs in the test set, while RITE-VAL has 581 sentence pairs in the training set and 1,200 sentence pairs in the test set. Though the data size of RITE2 has a little more, they are both small-scale datasets.

\begin{table}[ht!]
  \centering
  \begin{tabular}{|l|r|r|r|r|}
    \hline
    \multirow{2}{*}{Label} & \multicolumn{2}{c|}{RITE2} & \multicolumn{2}{c|}{RITE-VAL} \\
    \cline{2-5}
    & Training & Test & Training & Test \\ \hline
    B & 262 & 151 & 222 & 300 \\ \hline
    F & 544 & 328 & 148 & 300 \\ \hline
    C & 254 & 114 & 152 & 300 \\ \hline
    I & 261 & 288 & 59 & 300 \\ \hline
  \end{tabular}
  \caption{The label distribution of RITE2 and RITE-VAL.}
\end{table}

\paragraph{}
The table \ref{result:bfci_ntcir} and \ref{result:bc_ntcir} show the best results of RITE from NTCIR, both on the BFCI task and the BC task. The table \ref{result:bfci_liu_2016} and \ref{result:bc_liu_2016} show the best results of RITE of Liu et al., 2016.
% The best system of RITE2 is from Shih et al., 2013\cite{Shih2013IASLRS}.

\lstset{
  extendedchars=false,
  basicstyle=\ttfamily,
  keywordstyle=\color{blue},
  stringstyle=\color{purple},
  frame=lines,
  breaklines=true,
  showstringspaces=false,
  escapechar=\#,
}
% RITE2
\begin{figure}[ht!]
  \centering
  \caption{The examples of RITE2 and RITE-VAL}
  \begin{subfigure}{1\linewidth}
    \caption{Example of RITE2. The premise means "long-term use of steroids can cause emotional instability, hallucinations, and delusions" and the hypothesis means "steroids may cause hallucinations and delusions". The premise can inference the hypothesis but the hypothesis cannot inference the premise because it lack of the description of "emotional instability", so this example will be marked as "forward entailment".}
    \begin{minipage}{\linewidth}
    \begin{lstlisting}[language=XML]
    <pair id="430" label="F">
      <t1>#長期使用類固醇會導致情緒不穩，幻覺和妄想症#</t1>
      <t2>#類固醇可能造成幻覺妄想症#</t2>
    </pair>
    \end{lstlisting}
    \end{minipage}
    % 因為前提句
  \end{subfigure}

  % RITE-VAL
  \begin{subfigure}{1\linewidth}
    \caption{Example of the RITE-VAL training set, the column "category" show the relation of two sentences while the "吉力馬札羅火山" is an abbreviation of "吉力馬札羅山火山", so this pair is marked as "bi-directional".}
    \begin{minipage}{\linewidth}
    \begin{lstlisting}[language=XML]
    <pair id="1" label="B" revlabel="B" category="abbreviation">
      <t1>歷史上沒有吉力馬札羅山火山噴發的記錄。</t1>
      <t2>歷史上沒有吉力馬札羅火山噴發的記錄。</t2>
    </pair>
    \end{lstlisting}
    \end{minipage}
  \end{subfigure}
\end{figure}


\begin{table}[ht!]
  \centering
  \begin{tabular}{|l|r|r|r|r|r|}
  \hline
   & \multicolumn{1}{c|}{B-F1} & \multicolumn{1}{c|}{F-F1} & \multicolumn{1}{c|}{C-F1} & \multicolumn{1}{c|}{I-F1} & \multicolumn{1}{c|}{Macro F1} \\ \hline
  RITE-VAL & 0.4721 & 0.5206 & 0.4776 & 0.1511 & 0.4054 \\ \hline
  RITE-VAL-TEST & 0.6607 & 0.8296 & 0.4816 & 0.6185 & 0.6476 \\ \hline
  RITE2-TEST & 0.5235 & 0.6463 & 0.2990 & 0.3841 & 0.4632 \\ \hline
  \end{tabular}
  \caption{The best results of BFCI task from NTCIR-10 and NTCIR-11.}
  \label{result:bfci_ntcir}
\end{table}

\begin{table}[ht!]
  \centering
  \begin{tabular}{|l|r|r|r|}
  \hline
   & \multicolumn{1}{c|}{Y-F1} & \multicolumn{1}{c|}{N-F1} & \multicolumn{1}{c|}{Macro F1} \\ \hline
  RITE-VAL & 0.5672 & 0.5577 & 0.5624 \\ \hline
  RITE-VAL-TEST & 0.7894 & 0.6997 & 0.7446 \\ \hline
  RITE2-TEST & 0.7166 & 0.6263 & 0.6714 \\ \hline
  \end{tabular}
  \caption{The best results of BC task from NTCIR-10 and NTCIR-11.}
  \label{result:bc_ntcir}
\end{table}

\begin{table}[ht!]
  \centering
  \begin{tabular}{|l|r|r|r|r|r|}
  \hline
   & \multicolumn{1}{c|}{B-F1} & \multicolumn{1}{c|}{F-F1} & \multicolumn{1}{c|}{C-F1} & \multicolumn{1}{c|}{I-F1} & \multicolumn{1}{c|}{Macro F1} \\ \hline
  RITE-VAL & 0.6569 & 0.5438 & 0.5692 & 0.2209 & 0.4977 \\ \hline
  RITE-VAL-TEST & 0.5220 & 0.5592 & 0.5010 & 0.3623 & 0.4861 \\ \hline
  RITE2-TEST & 0.4720 & 0.6422 & 0.4052 & 0.4703 & 0.4974 \\ \hline
  \end{tabular}
  \caption{The best results of BFCI task from Liu et al., 2016.}
  \label{result:bfci_liu_2016}
\end{table}

\begin{table}[ht!]
  \centering
  \begin{tabular}{|l|r|r|r|}
  \hline
   & \multicolumn{1}{c|}{Y-F1} & \multicolumn{1}{c|}{N-F1} & \multicolumn{1}{c|}{Macro F1} \\ \hline
  RITE-VAL & 0.7425 & 0.5648 & 0.6536 \\ \hline
  RITE-VAL-TEST & 0.6766 & 0.5877 & 0.6321 \\ \hline
  RITE2-TEST & 0.7224 & 0.5992 & 0.6608 \\ \hline
  \end{tabular}
  \caption{The best results of BC task from Liu et al., 2016.}
  \label{result:bc_liu_2016}
\end{table}

\subsection{MNLI}
\paragraph{}
% MNLI 是一個透過 Crowd-sourced 建立的資料集，他蒐集了來自多種文體的句子，包含口說與書寫的文字。
MNLI (Multi-Genre Natural Language Inference, MultiNLI), as a dataset of GLUE benchmark, is a crowd-sourced large-scale dataset that collected sentences from a wide range of genres, both in spoken and written text. It has 392,702 sentence pairs in the training set and 10,000 in the dev set. Only the training set and dev set are public\footnote{https://cims.nyu.edu/~sbowman/multinli/}. The best system of MNLI is Raffel et al., 2020\cite{raffel2020exploring}, which reached an accuracy of 92.0\%.

\begin{table}[ht!]
  \centering
  \begin{tabular}{|c|r|r|r|}
  \hline
  \multicolumn{1}{|l|}{} & \multicolumn{1}{c|}{Entailment} & \multicolumn{1}{c|}{Contradiction} & \multicolumn{1}{c|}{Neutral} \\ \hline
  MNLI-Train & 130,899 & 130,903 & 130,900 \\ \hline
  MNLI-Dev & 3,479 & 3,213 & 3,123 \\ \hline
  \end{tabular}
  \caption{The label distribution of MNLI.}
\end{table}

\begin{figure}
\caption[An example of MNLI]{An example of MNLI. There are five labels listed that from different annotators, and a gold label indicates which label annotated the most.}
\begin{minipage}{\linewidth}
\begin{lstlisting}[language=Python]
{
  "annotator_labels": [
      "neutral",
      "entailment",
      "neutral",
      "neutral",
      "neutral"
  ],
  "genre": "slate",
  "gold_label": "neutral",
  "pairID": "63735n",
  "promptID": "63735",
  "sentence1": "The new rights are nice enough",
  "sentence1_binary_parse": "( ( The ( new rights ) ) ( are ( nice enough ) ) )",
  "sentence1_parse": "(ROOT (S (NP (DT The) (JJ new) (NNS rights)) (VP (VBP are) (ADJP (JJ nice) (RB enough)))))",
  "sentence2": "Everyone really likes the newest benefits ",
  "sentence2_binary_parse": "( Everyone ( really ( likes ( the ( newest benefits ) ) ) ) )",
  "sentence2_parse": "(ROOT (S (NP (NN Everyone)) (VP (ADVP (RB really)) (VBZ likes) (NP (DT the) (JJS newest) (NNS benefits)))))"
}
\end{lstlisting}
\end{minipage}
\end{figure}

\subsection{CNLI}
\paragraph{}
CNLI (Chinese Natural Language Inference) is a Simplified Chinese dataset from a sub task of The Seventeenth China National Conference on Computational Linguistics (CCL 2018)\footnote{http://www.cips-cl.org/static/CCL2018/index.html}. It has 90,000 sentence pairs in the training set, 10,000 in the dev set and 10,000 in the test set which are all public available on GitHub\footnote{https://github.com/blcunlp/CNLI}. To understand the impact of Traditional Chinese characters and Simplified Chinese characters, we make a Traditional Chinese version called CNLI-TW.

\paragraph{}
From the official evaluation results\footnote{https://git.io/JIlih}, the best system achieve an accuracy of 82.38\%, and the second best system achieve an accuracy of 78.28\%.

\begin{table}[ht!]
  \centering
  \begin{tabular}{|c|r|r|r|}
  \hline
             & Entailment & Contradiction & Neutral \\ \hline
  CNLI-Train & 2,9738     & 2,8937        & 3,1325  \\ \hline
  CNLI-Dev   & 3,485      & 3,417         & 3,098   \\ \hline
  CNLI-Test  & 3,475      & 3,343         & 3,182   \\ \hline
  \end{tabular}
  \caption{The label distribution of CNLI.}

\end{table}
\begin{figure}
\caption{An example of CNLI}
\begin{CJK*}{UTF8}{gbsn}
\begin{lstlisting}[language=Python, escapechar=\#]
{
  "pid": "AE5175",
  "t1": "#\color{purple}穿红衬衫的男人和拿着白色袋子的女人正在交谈。#",
  "t2": "#\color{purple}两个人在交谈#",
  "label": "entailment"
}
\end{lstlisting}
\end{CJK*}
\end{figure}

\subsection{OCNLI}
\paragraph{}
OCNLI (Original Chinese Natural Language Inference) is also a large-scale Simplified Chinese NLI dataset that does not rely on the automatic translation or non-expert annotation. The sentences are collected from government documents, news, literature, TV show transcripts, and telephone conversation transcripts. It has 50,486 sentence pairs in the training set, 3,000 in the dev set, and 3,000 in the test set. Only the training set and the dev set are fully labeled and public available\footnote{https://github.com/CLUEbenchmark/OCNLI}, the test set only contained sentence pairs. Same as CNLI, we also make a Traditional Chinese version called OCNLI-TW.

\paragraph{}
The best system of OCNLI from the verified official leaderboard\footnote{https://www.cluebenchmarks.com/nli.html} is RoBERTa which achieved an accuracy of 77.30\%, and the second-best system is BERT, which reached an accuracy of 71.25\%.

\begin{table}[ht!]
  \centering
  \begin{tabular}{|c|r|r|r|}
  \hline
              & Entailment & Contradiction & Neutral \\ \hline
  OCNLI-Train & 16,779     & 16,476        & 17,182  \\ \hline
  OCNLI-Dev   & 947        & 900           & 1,103   \\ \hline
  \end{tabular}
  \caption{The label distribution of OCNLI.}
\end{table}

\begin{figure}
\caption{An example of OCNLI}
\begin{minipage}{\linewidth}
\begin{CJK*}{UTF8}{gbsn}
\lstset{emph={null},emphstyle={\color{cyan}}}
\begin{lstlisting}[language=Python, escapechar=\#]
{
  "level": "medium",
  "sentence1": "#\color{purple}经济社会发展既有量的扩大,又有质的提升,为今后奠定了基础#",
  "sentence2": "#\color{purple}经济社会始终在向好的方向发展#",
  "label": "neutral",
  "label0": null,
  "label1": null,
  "label2": null,
  "label3": null,
  "label4": null,
  "genre": "gov",
  "prem_id": "gov_96",
  "id": 50434
}
\end{lstlisting}
\end{CJK*}
\end{minipage}
\end{figure}

\section{Approaches}
\subsection{Experiment Process}
\paragraph{}
% 我們將這部份分成特徵與分類器兩個部份來討論。在特徵的部份，我們重新製作了 Liu 等人提出的機器學習特徵，並提出了新的 CSA 方法製作新的特徵，同時也與一些常見的 NNLM 方法做比較。在分類器的部份，除了 Liu 等人使用的 SVM 分類器以外，也包含了單純的神經網路和 RNN 並結合 Attention 的機制，最後使用近年來在 NLP 領域相當受歡迎的 Transformer 模型 - BERT 做為分類器。
We will divide this part into the features and the classifiers to discuss. In the features, we remade the machine learning features mentioned in Liu et al., and proposed a new method called ``cosine similarity alignment''. Meanwhile, we compared these methods with some common language models. In the classifiers, in addition to the SVM classifier mentioned in Liu et al., we also used the simple neural network and the RNN with attention mechanism. Finally, we will use a transformer model, BERT, which is popular in the NLP domain in recent years.

\paragraph{}
% 為了使用 Deep Learning 解決 NLI 問題並且與傳統機器學習方法做比較，我們首先重製了先前 Liu 等人使用的 ML 特徵與 SVM 的實驗，然後測試將 SVM 分類器改成簡單的 DNN 的效能如何。接下來，我們提出了 CSA 的方法，並將其放入深度學習的模型裡面。最後嘗試透過最近在 NLP 領域相當受歡迎的 BERT Transformer 模型來提升整體效能。
To solve the NLI problem by deep learning and compare the performance to the traditional machine learning methods, we first reproduce the machine learning feature extraction and the SVM experiment of Liu et al., then we test the performance of replacing the SVM classifier with a simple DNN classifier. And we propose a new method called ``Cosine Similarity Alignment'' that generate alignment features using cosine similarity of word vectors and feed it into RNN-attention neural network. Finally, we try to use the BERT transformer model which becomes popular in the NLP domain in recent years.

\paragraph{}
% 在重製傳統機器學習的部分，我們首先製作了在 Liu 等人 (2016) 提到的 Machine Learning Features，不過當時主要針對中文資料集來產生特徵，且使用的部分資源像是中文 WordNet 等已過於老舊，難以適用於現今的資料集，所以我們稍微修改了一部份的特徵產生方式，並同時設計了針對英文資料集的相同特徵。其中有部分特徵需要用到 Word Vectors 來產生，所以我們也使用了不同的 NNLM 來測試效果如何。
To reproduce the experiment of traditional machine learning, we first reproduce the features mentioned in Liu et al. (2016). But these features can only be applied to the Traditional Chinese datasets, and some resources like Traditional Chinese WordNet are too old to fit with the datasets nowadays. So we modified a little part of feature extraction and make the method compatible with the English datasets. Some of the features need to use the word vectors, so we use different language models to test the performance.

\paragraph{}
% 在 BERT 的實驗中，我們主要使用官方釋出的 bert-base-chinese 與 bert-multilingual-cased 兩個模型來進行實驗，並透過不同的資料集進行預訓練，最後再用 RITE 做 fine-tune。
In the experiment of BERT, we use the official released models and pre-training with different datasets then fine-tuning with the RITE datasets. At last, we try to merge the machine learning features and the cosine similarity alignment features into the BERT model.


\subsection{Machine Learning Features} \label{Features}
\paragraph{}
In this section, we will describe the features mentioned in Liu et al.\cite{liu_2016} and reproduce it. These features are used for machine learning originally, so we called these features as ``\textbf{ML Features}''.

\subsubsection{N-grams Features}
% Character N-gram Overlap & Word N-gram Overlap.
% Use CKIP-Tagger to do word segmentation.
\paragraph{}
% 讓 ngrams_1 and ngrams_2 為 t1 和 t2 所有的 n-grams，在只考慮內容詞的情況下計算 n-gram 一致的比例，同時也計算字與詞的版本
Let $ngrams_1$ and $ngrams_2$ be all n-grams of $t_1$ and $t_2$, calculate the proportion of n-grams overlap that are consistent with only content words, both the version of the words and the characters are calculated.

\begin{equation}
  overlap_1(ngrams_1,ngrams_2)=\frac{|ngrams_1\cap ngrams_2|}{|ngrams_1|}
\end{equation}

\begin{equation}
  overlap_2(ngrams_1,ngrams_2)=\frac{|ngrams_1\cap ngrams_2|}{|ngrams_2|}
\end{equation}
\begin{description}
  \item[ ] The features of n-grams overlap are listed below:
  \begin{description}
    \item[ 1.] n-grams overlap of characters $(t_1)$
    \item[ 2.] n-grams overlap of characters $(t_2)$
    \item[ 3.] n-grams overlap of words $(t_1)$
    \item[ 4.] n-grams overlap of words $(t_2)$
  \end{description}
\end{description}

\subsubsection{Grammer Features}
\begin{description}
  \item[ ] To generate grammar features, we use Stanza\cite{qi2020stanza} as the interface to Stanford Parser to parse the dependency of the sentences. Each dependency is composed of one relation and two arguments: the head and the modifier. These features are defined in Liu et al. (2016) and listed below:
  \begin{description}
    \item[ 5.] the proportion of the same relations $(t_1)$
    \item[ 6.] the proportion of the same relations $(t_2)$
    \item[ 7.] the proportion of the different relations $(t_1)$
    \item[ 8.] the proportion of the different relations $(t_2)$
    \item[ 9.] the proportion of the a same relation and a same argument but the other argument is different $(t_1)$
    \item[10.] the proportion of the a same relation and a same argument but the other argument is different $(t_2)$
    \item[11.] the proportion of the same arguments but different relation $(t_1)$
    \item[12.] the proportion of the same arguments but different relation $(t_2)$
    \item[13.] the proportion of the lack of the head $(t_1)$
    \item[14.] the proportion of the lack of the head $(t_2)$
    \item[15.] the proportion of the lack of the relation $(t_1)$
    \item[16.] the proportion of the lack of the relation $(t_2)$
  \end{description}
\end{description}

\subsubsection{Semantic Features}
\paragraph{}
Using NLTK\cite{nltk} with WordNet\cite{wordnet} to calculate WUP similarity\cite{wu-palmer-1994-verb}.
\begin{equation}
  wup(s_1,s_2)=\frac{2\times depth(lcs)}{depth(s_1)+depth(s_2)}
\end{equation}

\paragraph{}
% 我們可以使用 NLTK 來找到 LCS，並透過節點的深度計算出兩個 synsets 的相似度。因為每個內容詞可能有不只一個 synsets，所以我們定義兩個內容詞 c1 和 c2 的 wup 相似度如下
We can find the LCS (least common subsumer) with NLTK and calculate the depth of synsets to get WUP similarity. Because each content word may has more than one synset, we defined WUP similarity of content words $c_1$ and $c_2$ as following.

\begin{equation}
  sim(c_1,c_2)=\max\limits_{\substack{s_1\in synsets(c_1) \\ s_2\in synsets(c_2)}} wup(s_1,s_2)
\end{equation}

\paragraph{}
Harbin Institute of Technology Information Retrieval Lab\footnote{http://ir.hit.edu.cn/} Chinese Synonym Forest (or Tongyici Cilin) [Extended] lv5 Similarity

\begin{equation}
  lv5(C_1,C_2)=\frac{1}{|C_1|}\sum\limits_{c_1\in C_1}\max\limits_{c_2\in C_2} sim(c_1,c_2)
\end{equation}

\paragraph{}
% 因為中文的部份缺少完善的 WordNet 資源，而英文的部份則是缺少同義詞詞林的資源，所以這部份會透過 Cosine Similarity 來取代。
Because lack of a proper WordNet resource in Chinese and synonym forest in English, it is unable to calculate WUP similarity in Chinese and lv5 similarity in English. They will be replaced with the cosine similarity.

\begin{equation}
  similarity=\frac{A\cdot B}{||A||\ ||B||}=\frac{\sum\limits^{n}_{i=1}A_iB_i}{\sqrt{\sum\limits^{n}_{i=1}A^2_i}\sqrt{\sum\limits^{n}_{i=1}B^2_i}}
\end{equation}

\begin{description}
  \item[ ] The semantic features in English is listed below:
  \begin{description}
    \item[17.] average of cosine similarity $(t_1)$
    \item[18.] average of cosine similarity $(t_2)$
    \item[19.] average of WUP similarity $(t_1)$
    \item[20.] average of WUP similarity $(t_2)$
  \end{description}
\end{description}

\begin{description}
  \item[ ] The semantic features in Chinese is listed below:
  \begin{description}
    \item[17.] average of cosine similarity $(t_1)$
    \item[18.] average of cosine similarity $(t_2)$
    \item[19.] average of lv5 similarity $(t_1)$
    \item[20.] average of lv5 similarity $(t_2)$
  \end{description}
\end{description}

\subsection{Language Model}
\paragraph{}
The language models

\subsubsection{Self-trained Word Embedding}
\paragraph{}
Description of self-trained word embedding.

\subsubsection{fastText}
\paragraph{}
fastText\cite{bojanowski2016enriching} is a library for efficient text classification and representation learning created by Facebook's AI Research (FAIR) lab. This library allows us to create word embedding with unsupervised learning algorithms, like skip-gram and continuous bag-of-word (CBOW). With this library, we train a word representation from Traditional Chinese Wikipedia, both in the version of skip-gram and CBOW, the embedding size is 250.

\subsubsection{Google NNLM}
\paragraph{}
Google NNLM is provided by TensorFlow Hub\footnote{https://www.tensorflow.org/hub} which has many different language models that already be trained by a large corpus. We use the Chinese version of NNLM that trained on Simplified Chinese Google News 100B corpus and publish by Google\footnote{https://tfhub.dev/google/tf2-preview/nnlm-zh-dim128/1}. In order to fit the Traditional Chinese dataset, we convert the dictionary of the Google NNLM from Simplified Chinese into Traditional Chinese.
% BERT Language Model

\subsection{Cosine Similarity Alignment}
\paragraph{}
We propose a new method to do alignment by calculating the cosine similarity between words of two sentences and set a ``similarity threshold'' to define if the two words will be aligned. The similarity threshold is variable and we will observe the impact of the change of the value of the similarity threshold.

\paragraph{}
% 首先先計算兩句話的每個詞之間的 Cosine Similarity 建立一個 Similarity Table，然後從表中挑出相似數值最高的兩個詞，將 premise 的詞放入 list1 並將 hypothesis 的詞放入 list2，然後將這兩個詞從表中刪去，再繼續往下挑相似數值最高的兩個詞，直到該相似數值小於 Similarity Threshold 為止。接下來把 premise 剩下的詞一個一個放入 list1，每放一個詞到 list1 裡面，就在 list2 裡面放一個零，hypothesis 剩下的詞也是一樣的操作。最後將 list2 的值減去 list1 獲得最後的 CSA Features。
First, we calculated the cosine similarity of each word of two sentences to build a similarity table, then picked up the word pair with the highest value of similarity, and put the word vector of the word from the premise sentence into $list_1$ and put the one from the hypothesis sentence into $list_2$ and remove the two words from the similarity table. Repeat this step until the highest value of the similarity is lower than the similarity threshold. Next, we put the word vectors of the left words of the premise sentence into $list_1$ one by one, every time we put a word vector into $list_1$, put a zero vector into $list_2$, and do the same operation to the left words of the hypothesis sentence, so the two lists will remain the same size. After all, we use $list_2$ to minus $list_1$ to obtain the final ``CSA Features''.

\subsection{Classifier}

\subsubsection{Neural Network}
\paragraph{}
% Simple DNN 模型是只使用了一層 Dense Layer 的模型，透過 Tensorflow 套件建立而成。這個模型架構設計很單純，只用來接收 ML Features。
A simple DNN model is a model only using a dense layer, building by Tensorflow API. This model architecture is pretty simple, only use to receive machine learning features.

\subsubsection{RNN-Attention Model}
\paragraph{}
% RNN-Attention 模型是數種由不同數量與順序的 RNN Layers 與 Attention Layers 組合而成的模型
The RNN-Attention Model (RA Model) is a combination of several different numbers and orders of RNN layers and attention layers.

The attention mechanism is described in Vaswani et al. (2017)

\subsubsection{BERT}
\paragraph{}
The structure of transformer\cite{vaswani2017attention} has been described in Vaswani et al., 2017, they use multi-head attention to build the encoder-decoder model, which uses to solve the machine translation tasks. On the other hand, BERT\cite{devlin2018bert} (Devlin et al., 2019) also used the transformer architecture to achieve excellent performance on several natural language processing tasks, including MNLI, QNLI, and RTE which are contained in GLUE benchmark. The source code of BERT are available on GitHub\footnote{https://github.com/google-research/bert} and it can be easily called using HuggingFace Transformers API\cite{wolf-etal-2020-transformers}.

\paragraph{}
There are many different models from the official GitHub repository, like the \texttt{bert-base-cased} model is pre-trained by cased English corpus and the \texttt{bert-base-chinese} model is pre-trained by both Traditional Chinese and Simplified Chinese corpus. Besides these language-specific models, the most important model is the cross-lingual model like the \texttt{bert-base-multilingual-cased} model, which allows us to train and fine-tune with different languages in one model easily.

\section{Experiments}
\subsection{Evaluation Metrics}
\paragraph{}
% 根據 NTCIR 主辦單位提供的正式評估公式來計算 macroF1
According to the formal evaluation formula provided by NTCIR:

\begin{equation}
  macroF1=\frac{1}{|C|}\sum_{c\in C}F1_c
\end{equation}

\paragraph{}
$C$ is all categories, and $c$ is one of the categories. Because $macroF1$ is the average of F1-Score to each category, it won't be affected by a category that has a large number. The experiments of the BFCI task will use this evaluation formula.

\paragraph{}
F-measure also called F1-score, is a measure that is calculated from precision and recall. It is usually used in the information retrieval field to compare the different systems. F1-score, precision, and recall are defined as follows.

\begin{equation}
  F1=\frac{2\times Precision\times Recall}{Precision+Recall}
\end{equation}

\begin{equation}
  Precision=\frac{N_{correct}}{N_{predicted}}
\end{equation}

\begin{equation}
  Recall=\frac{N_{correct}}{N_{target}}
\end{equation}

\paragraph{}
To compare the experiments of the ECN tasks with the other systems, we use accuracy as the evaluation formula of the ECN tasks.

\subsection{Datasets}
\paragraph{}
To understand the impact of the characters of the Traditional and the Simplified, we use OpenCC\footnote{https://github.com/BYVoid/OpenCC} which available as a python package to convert CNLI and OCNLI into CNLI-TW and OCNLI-TW.

\subsection{Machine Learning}
\paragraph{}
To cross-compare the performance of datasets, we first extended the RITE-VAL with reversed labels to generate RITE-VAL-REV and mixed RITE2 with RITE-VAL-REV to generate RITE-VAL-REV-2. So we have RITE2, RITE-VAL, RITE-VAL-REV, and RITE-VAL-REV-2 to be used as the training sets. The test sets of RITE2 and RITE-VAL will be marked as RITE2-TEST and RITE-VAL-TEST.

\paragraph{}
% SVM 的實驗結果：Kernel Comparison
The Scikit-Learn package provides four types of kernels of SVM: RBF, Linear, Sigmoid, and Poly. We want to figure out which kernel is the most suitable, so we use the ML features mentioned in \ref{Features} to compare the performance of kernels of SVM. From table \ref{svm_kernel} we can see that both in RITE-VAL and RITE2, RBF kernel is the most suitable kernel of SVM, so we use RBF kernel as the default kernel in the following experiments.

\begin{table}[ht!]
  \centering
  \begin{tabular}{|l|r|r|r|r|}
  \hline
  \multicolumn{5}{|c|}{SVM Kernels Comparison} \\ \hline
  \multicolumn{1}{|c|}{Kernel} & \multicolumn{1}{c|}{RBF} & \multicolumn{1}{c|}{Linear} & \multicolumn{1}{c|}{Sigmoid} & \multicolumn{1}{c|}{Poly} \\ \hline
  \multicolumn{5}{|c|}{Validation Target: RITE-VAL} \\ \hline
  all & 0.4011 & 0.3538 & 0.2696 & 0.3495 \\ \hline
  all - lex & 0.4047 & 0.3449 & 0.3434 & 0.3510 \\ \hline
  all - syn & 0.4045 & 0.3567 & 0.2701 & 0.3547 \\ \hline
  all - wn & \textbf{0.4122} & 0.3560 & 0.3090 & 0.3539 \\ \hline
  all - cl & 0.3978 & 0.3539 & 0.3043 & 0.3445 \\ \hline
  \multicolumn{5}{|c|}{Validation Target: RITE2} \\ \hline
  all & 0.5177 & 0.4872 & 0.3126 & 0.4949 \\ \hline
  all - lex & 0.4868 & 0.4555 & 0.3176 & 0.4712 \\ \hline
  all - syn & 0.4997 & 0.4923 & 0.2509 & 0.4741 \\ \hline
  all - wn & 0.4941 & 0.4766 & 0.3158 & 0.4902 \\ \hline
  all - cl & \textbf{0.5410} & 0.4871 & 0.3172 & 0.4928 \\ \hline
  \end{tabular}
  \caption{Results of SVM kernels comparison, the training data is RITE-VAL-REV-2.}
  \label{svm_kernel}
\end{table}

\subsection{SVM and Simple DNN}
\paragraph{}
% 當 Training Source 是 RITE-VAL 的時候，Simple DNN 的效能可能不見得是最好的，我們認為這可能是因為 RITE-VAL 的資料規模相對較小，在資料規模不大的情況下 SVM 可以取得較好的效果。
We compare the performance of SVM and our simple DNN model. The hidden size of the hidden layer is 512, the optimizer is RMSprop, the learning rate is 5e-5, the batch size is 8, and training for 500 epochs. From table \ref{tab:svm_simplednn} we can see that the simple DNN has better performance in almost all cases, but when the training source is RITE-VAL, it seems that the simple DNN is not always the best. We think this may because of the relatively small-scale data size of RITE-VAL. When the data size is not large enough, the SVM will get better performance usually.

\begin{landscape}
\topskip0pt
\vspace*{\fill}
\begin{table}[ht!]
  \centering
  \begin{tabular}{|l|r|r|r|r|r|r|r|r|}
  \hline
   & \multicolumn{4}{c|}{SVM} & \multicolumn{4}{c|}{Simple DNN} \\ \hline
  Training Source & \multicolumn{1}{l|}{RITE2} & \multicolumn{1}{l|}{RITE-VAL} & \multicolumn{1}{l|}{RITE-VAL-REV} & \multicolumn{1}{l|}{RITE-VAL-REV-2} & \multicolumn{1}{l|}{RITE2} & \multicolumn{1}{l|}{RITE-VAL} & \multicolumn{1}{l|}{RITE-VAL-REV} & \multicolumn{1}{l|}{RITE-VAL-REV-2} \\ \hline
  Validation Target & \multicolumn{8}{c|}{RITE-VAL} \\ \hline
  all & 0.4203 & \textbf{0.3840} & 0.3834 & 0.4011 & \textbf{0.4665} & 0.4107 & 0.4483 & \textbf{0.4640} \\ \hline
  all - lex & 0.3864 & 0.3701 & 0.3704 & 0.4047 & 0.4479 & \textbf{0.4146} & 0.4374 & 0.4494 \\ \hline
  all - syn & 0.4100 & 0.3673 & 0.3747 & 0.4045 & 0.4355 & 0.3726 & 0.3977 & 0.4037 \\ \hline
  all - wn & \textbf{0.4290} & 0.3829 & \textbf{0.3898} & \textbf{0.4122} & 0.4641 & 0.4134 & \textbf{0.4486} & 0.4554 \\ \hline
  all - cl & 0.4211 & 0.3746 & 0.3498 & 0.3978 & 0.4653 & 0.4098 & 0.4416 & 0.4554 \\ \hline
  Validation Target & \multicolumn{8}{c|}{RITE2} \\ \hline
  all & 0.5220 & 0.3639 & \textbf{0.4528} & 0.5177 & 0.5936 & 0.3655 & \textbf{0.5359} & 0.5723 \\ \hline
  all - lex & \textit{0.4913} & 0.3733 & 0.4327 & \textit{0.4868} & 0.5654 & 0.3590 & 0.5191 & 0.5498 \\ \hline
  all - syn & \textbf{0.5572} & \textit{0.3289} & \textit{0.4173} & 0.4997 & 0.5516 & 0.3457 & 0.4775 & 0.5147 \\ \hline
  all - wn & 0.4965 & 0.3441 & 0.4431 & 0.4941 & 0.5820 & 0.3598 & 0.5252 & 0.5700 \\ \hline
  all - cl & 0.5450 & \textbf{0.3864} & 0.4368 & \textbf{0.5410} & \textbf{0.5954} & \textbf{0.3752} & 0.5358 & \textbf{0.5792} \\ \hline
  Validation Target & \multicolumn{8}{c|}{RITE-VAL-TEST} \\ \hline
  all & 0.3427 & \textbf{0.3522} & \textbf{0.3872} & 0.3900 & 0.3715 & 0.3506 & 0.3862 & 0.4020 \\ \hline
  all - lex & 0.3537 & 0.3428 & 0.3165 & 0.3570 & 0.3584 & 0.3504 & 0.3789 & 0.3857 \\ \hline
  all - syn & 0.3375 & 0.2839 & 0.3125 & 0.3227 & 0.3016 & 0.2822 & 0.3060 & 0.3182 \\ \hline
  all - wn & 0.3435 & 0.3500 & 0.3683 & \textbf{0.3903} & \textbf{0.3709} & \textbf{0.3523} & \textbf{0.3785} & \textbf{0.3941} \\ \hline
  all - cl & \textbf{0.3598} & 0.3294 & 0.3865 & 0.3899 & 0.3702 & 0.3516 & 0.3902 & 0.4138 \\ \hline
  Validation Target & \multicolumn{8}{c|}{RITE2-TEST} \\ \hline
  all & 0.4527 & 0.3413 & 0.4157 & 0.4365 & 0.4512 & 0.3261 & 0.4287 & 0.4742 \\ \hline
  all - lex & 0.4119 & 0.3319 & 0.4114 & 0.4416 & 0.4321 & 0.3130 & 0.4189 & 0.4394 \\ \hline
  all - syn & 0.4386 & 0.3455 & 0.4315 & 0.4318 & 0.4478 & 0.3033 & 0.4215 & 0.4457 \\ \hline
  all - wn & 0.4566 & 0.3273 & \textbf{0.4354} & \textbf{0.4618} & \textbf{0.4488} & \textbf{0.3197} & \textbf{0.4252} & \textbf{0.4663} \\ \hline
  all - cl & \textbf{0.4616} & \textbf{0.3559} & 0.4249 & 0.4497 & 0.4625 & 0.3196 & 0.4266 & 0.4717 \\ \hline
  \end{tabular}
  \caption{Results of SVM and simple DNN comparison.}
  \label{tab:svm_simplednn}
\end{table}
\vspace*{\fill}
\end{landscape}

\subsection{BERT}
\paragraph{}
We first reproduce the performance of BERT on the MNLI dataset. In this experiment, the \texttt{bert-base-uncased} is used, the learning rate is 3e-5. After 2 epochs of training, the accuracy of the dev set is 0.8346, which is close to the original paper (0.846). Then we reproduce the same experiment of BERT on the OCNLI dataset but replace with \texttt{bert-base-chinese} model, the accuracy of the dev set is 0.7447, which is also close to the original paper (0.745).

% 成功重製以上兩個實驗的效能後，我們將相同的實驗設定套用到 CNLI 上。CNLI 的 dev set 準確率為 0.7830 而 test set 準確率為 0.7832。雖然並不如 CCL 2018 會議上最好的系統 (0.8238) 但是比第二名的系統好 (0.7828)。
\paragraph{}
After reproducing these two experiments above successfully, we applied the same experiment on the CNLI dataset. The accuracy of the dev set of CNLI is 0.7830, and the accuracy of the test set is 0.7832. Though the performance is not better than the best system in CCL 2018 which reached 0.8238, the performance is better than the second-best system (0.7828).

\paragraph{}
% 接下來，我們開始關注當 RITE 資料集做為 validation set 並使用其他大型 NLI 資料集做 pre-training 時，BERT 的效能如何。因為其他大型 NLI 資料集皆為 ECN task 而 RITE 為 BFCI task，所以我們會先用訓練資料集訓練一個 ECN Model，然後將訓練資料集的 premise 與 hypothesis 對調再進行 prediction，若對調前的 label 為 entailment 而且對調後的 prediction 也是 entailment，就將這組 sentence pair 標記為 bi-directional，由此產生 BFCI 版本的訓練資料集，再以此版本資料集作為訓練資料集。
Next, we focus on how the performance of BERT will be when the RITE dataset is validation set and using other large-scale NLI datasets as the training set. Because the task type of other large-scale NLI datasets are ECN task, but the task type of RITE is BFCI task, we need to train an ECN model first, then we swap the premise and the hypothesis of the training set and do the prediction. If the original label and the prediction of the swapped sentence pair are both ``entailment'', then we mark this sentence pair as ``bi-directional'', so we can generate the BFCI version of the dataset and use this version of the dataset as the training set.

\paragraph{}
% 首先我們以 CNLI 做為 pre-training data 並以 RITE 進行 fine-tune。
We use the CNLI dataset as the training data first, then test on RITE2 and RITE-VAL directly. The BERT model we fine-tuned is \texttt{bert-base-chinese} The result is shown in table \ref{result:bert_cnli}, we can see that the performance of the test set is much better than the best system in either Liu et al or NTCIR.

\begin{table}[ht!]
  \centering
  \begin{tabular}{|l|r|r|r|r|r|}
  \hline
   & \multicolumn{1}{c|}{B-F1} & \multicolumn{1}{c|}{F-F1} & \multicolumn{1}{c|}{C-F1} & \multicolumn{1}{c|}{I-F1} & \multicolumn{1}{c|}{Macro F1} \\ \hline
  RITE-VAL & 0.7024 & 0.5022 & 0.4863 & 0.2157 & 0.4766 \\ \hline
  RITE-VAL-TEST & 0.6110 & 0.5506 & 0.5219 & 0.3135 & 0.4993 \\ \hline
  RITE2 & 0.5443 & 0.7003 & 0.4158 & 0.4902 & 0.5377 \\ \hline
  RITE2-TEST & 0.6797 & 0.6667 & 0.5035 & 0.5303 & 0.5950 \\ \hline
  \end{tabular}
  \caption{Results of BERT that train with CNLI and directly test with RITE.}
  \label{result:bert_cnli}
\end{table}

\paragraph{}
% 藉由上個實驗的成功，我們進一步的將 RITE 切成 10-fold 並納入 fine-tune 的環節，我們將 CNLI 訓練出來的模型透過每個 fold 的 training set 再進行一次訓練，並測試該 fold 的 test set，最後將每個 fold 的測試集結果合併在一起進算 micro f1。得到的結果如表 x，其效能有大幅度的提昇。
With the success of the previous experiment, we split the RITE dataset into fixed 10-folds and bring it into the fine-tuning. We use the training set of each fold to train after the model that trained by the CNLI-TW dataset and test on the test set of the fold, then we merge the results of all folds and calculate the micro f1-score. The result is shown in table \ref{result:bert_cnli_transfer}, the performance has a huge improvement.

\begin{table}[ht!]
  \centering
  \begin{tabular}{|l|r|r|r|r|r|}
  \hline
   & \multicolumn{1}{c|}{B-F1} & \multicolumn{1}{c|}{F-F1} & \multicolumn{1}{c|}{C-F1} & \multicolumn{1}{c|}{I-F1} & \multicolumn{1}{c|}{Macro F1} \\ \hline
  RITE-VAL & 0.7384 & 0.6754 & 0.6224 & 0.3299 & 0.5915 \\ \hline
  RITE-VAL-TEST & 0.6084 & 0.6172 & 0.5764 & 0.2565 & 0.5146 \\ \hline
  RITE2 & 0.6827 & 0.8444 & 0.5493 & 0.6627 & 0.6848 \\ \hline
  RITE2-TEST & 0.7140 & 0.7393 & 0.5315 & 0.5856 & 0.6426 \\ \hline
  \end{tabular}
  \caption{Results of the BERT that train with CNLI-TW and fine-tune with RITE.}
  \label{result:bert_cnli_transfer}
\end{table}

\paragraph{}
We also compared the performance between CNLI and CNLI-TW, the result of CNLI is shown in table \ref{result:bert_cnli_cn}. Compare to table \ref{result:bert_cnli_transfer}, we can see that the performance of CNLI-TW is better than CNLI, so we think the dataset composed of Traditional Chinese characters is more suitable for RITE, even though the dataset is augmented from the translation tool.

\begin{table}[ht!]
  \centering
  \begin{tabular}{|l|r|r|r|r|r|}
  \hline
   & \multicolumn{1}{c|}{B-F1} & \multicolumn{1}{c|}{F-F1} & \multicolumn{1}{c|}{C-F1} & \multicolumn{1}{c|}{I-F1} & \multicolumn{1}{c|}{Macro F1} \\ \hline
  RITE-VAL & 0.6881 & 0.6424 & 0.5893 & 0.3238 & 0.5609 \\ \hline
  RITE-VAL-TEST & 0.5316 & 0.6077 & 0.5579 & 0.2936 & 0.4977 \\ \hline
  RITE2 & 0.6718 & 0.8408 & 0.5167 & 0.6605 & 0.6724 \\ \hline
  RITE2-TEST & 0.6796 & 0.7336 & 0.4811 & 0.6228 & 0.6293 \\ \hline
  \end{tabular}
  \caption{Results of the BERT that train with CNLI-CN and fine-tune with RITE.}
  \label{result:bert_cnli_cn}
\end{table}

\paragraph{}
% 根據上述的實驗結果顯示，資料集的規模對模型效能有巨大的影響，而正確的 fine-tune 步驟也起了關鍵的作用。所以我們下一步決定嘗試使用 MNLI 資料集來訓練 BERT 的跨語言模型，並用相同的步驟以 RITE 進行 fine-tune。
According to the results of the above experiments, the scale of the training datasets has a huge impact on performance, and the correct steps of fine-tuning are also the key part. So we decided to use the MNLI dataset to train the cross-lingual BERT model (\texttt{bert-multilingual-cased}) and fine-tuned with the RITE dataset with the same steps. The result is shown in table \ref{result:bert_mnli_transfer}, we can see the performance has a little improvement.

\begin{table}[ht!]
  \centering
  \begin{tabular}{|l|r|r|r|r|r|}
  \hline
   & \multicolumn{1}{c|}{B-F1} & \multicolumn{1}{c|}{F-F1} & \multicolumn{1}{c|}{C-F1} & \multicolumn{1}{c|}{I-F1} & \multicolumn{1}{c|}{Macro F1} \\ \hline
  RITE-VAL & 0.7478 & 0.6689 & 0.5567 & 0.3119 & 0.5713 \\ \hline
  RITE-VAL-TEST & 0.6137 & 0.6291 & 0.6122 & 0.3696 & 0.5561 \\ \hline
  RITE2 & 0.6867 & 0.8640 & 0.5247 & 0.6920 & 0.6919 \\ \hline
  RITE2-TEST & 0.7834 & 0.7881 & 0.5757 & 0.6955 & 0.7107 \\ \hline
  \end{tabular}
  \caption{Results of the BERT that train with MNLI and fine-tune with RITE.}
  \label{result:bert_mnli_transfer}
\end{table}

\paragraph{}
% 我們同樣進行了以 OCNLI 為訓練資料的實驗，OCNLI 的句子品質相較於 CNLI 而言較高一點，但其資料規模並不如 CNLI。以同樣的方法進行實驗之後，其實驗結果如表。結果顯示，以 OCNLI 為訓練集的實驗，無論在 RITE2 或 RITE-VAL 的實驗上，效能皆不如以 CNLI 作為訓練集的實驗。
We also do the experiment that uses OCNLI as the training set. The quality of sentences of OCNLI seems higher than CNLI, but the scale of data size is lower. The result is shown in table \ref{result:bert_ocnli_transfer}, and we can see, compared to the experiment of CNLI, the performance is lower no matter on RITE2 or RITE-VAL.

\begin{table}[ht!]
  \centering
  \begin{tabular}{|l|r|r|r|r|r|}
  \hline
   & \multicolumn{1}{c|}{B-F1} & \multicolumn{1}{c|}{F-F1} & \multicolumn{1}{c|}{C-F1} & \multicolumn{1}{c|}{I-F1} & \multicolumn{1}{c|}{Macro F1} \\ \hline
  RITE-VAL & 0.6870 & 0.6667 & 0.4948 & 0.2778 & 0.5316 \\ \hline
  RITE-VAL-TEST & 0.5608 & 0.6091 & 0.5517 & 0.3153 & 0.5092 \\ \hline
  RITE2 & 0.6416 & 0.8406 & 0.5209 & 0.6810 & 0.6710 \\ \hline
  RITE2-TEST & 0.6488 & 0.7120 & 0.4000 & 0.5928 & 0.5884 \\ \hline
  \end{tabular}
  \caption{Results of the BERT that train with OCNLI and fine-tune with RITE.}
  \label{result:bert_ocnli_transfer}
\end{table}

\paragraph{}
% 因為跨語言實驗的提昇，所以我們想試試看混合語言的效果如何。我們使用兩種方法來進行混合語言的實驗：第一個是先用 MNLI 訓練，再用 CNLI 訓練：第二個是將 MNLI 與 CNLI 的訓練集混合在一起變成新的訓練資料集，我們稱之為 Mixed-NLI，然後進行訓練。前者的實驗結果如表X，後者的實驗結果如表Y。與使用 MNLI 做為訓練資料的實驗相比，效能的改進幅度相當的小，甚至是降低了一點，但差距幾乎都在 0.1% 以內。所以我們認為在資料規模超過 MNLI 之後能獲得的效能提昇相當有限。
Due to the improvement of the cross-lingual experiment, we want to know the performance of the mix-lingual. We use two ways to achieve the mix-lingual experiment. The first is to train with MNLI, and then train with CNLI. The second is to mix MNLI and CNLI into a new training set we called Mixed-NLI. The results are shown in table \ref{result:bert_mnli_cnli} and table \ref{result:bert_mixed_nli}, as we can see, compared to the experiment of MNLI, both of the improvement of the two experiments are quite small and even decreases on RITE2, but the differences are within 0.1\%. So we think when the scale of data size is over than the size of MNLI, the improvement of performance is quite limited.

\begin{table}[ht!]
  \centering
  \begin{tabular}{|l|r|r|r|r|r|}
  \hline
   & \multicolumn{1}{c|}{B-F1} & \multicolumn{1}{c|}{F-F1} & \multicolumn{1}{c|}{C-F1} & \multicolumn{1}{c|}{I-F1} & \multicolumn{1}{c|}{Macro F1} \\ \hline
  RITE-VAL & 0.7522 & 0.6839 & 0.5895 & 0.2991 & 0.5811 \\ \hline
  RITE-VAL-TEST & 0.6291 & 0.6365 & 0.6163 & 0.3780 & 0.5650 \\ \hline
  RITE2 & 0.6906 & 0.8551 & 0.5520 & 0.6830 & 0.6952 \\ \hline
  RITE2-TEST & 0.7689 & 0.7748 & 0.5662 & 0.6721 & 0.6955 \\ \hline
  \end{tabular}
  \caption{Results of the BERT that train with MNLI first then train with CNLI, and fine-tune with RITE.}
  \label{result:bert_mnli_cnli}
\end{table}

\begin{table}[ht!]
  \centering
  \begin{tabular}{|l|r|r|r|r|r|}
  \hline
   & \multicolumn{1}{c|}{B-F1} & \multicolumn{1}{c|}{F-F1} & \multicolumn{1}{c|}{C-F1} & \multicolumn{1}{c|}{I-F1} & \multicolumn{1}{c|}{Macro F1} \\ \hline
  RITE-VAL & 0.7505 & 0.6537 & 0.5866 & 0.3670 & 0.5895 \\ \hline
  RITE-VAL-TEST & 0.6326 & 0.6268 & 0.6042 & 0.3731 & 0.5592 \\ \hline
  RITE2 & 0.6962 & 0.8455 & 0.5322 & 0.6779 & 0.6880 \\ \hline
  RITE2-TEST & 0.7731 & 0.7936 & 0.5604 & 0.6875 & 0.7036 \\ \hline
  \end{tabular}
  \caption{Results of the BERT that train with Mixed-NLI and fine-tune with RITE.}
  \label{result:bert_mixed_nli}
\end{table}

\paragraph{}
We also feed the machine learning features mentioned in \ref{Features} into BERT, concatenating with the pooled outputs of BERT. The result is shown in table \ref{result:bert_mnli_ft}. However, the performance is not increased, so we think the traditional feature is not helpful to the deep learning models of this experiment.

\begin{table}[]
  \centering
  \begin{tabular}{|l|r|r|r|r|r|}
  \hline
   & \multicolumn{1}{c|}{B-F1} & \multicolumn{1}{c|}{F-F1} & \multicolumn{1}{c|}{C-F1} & \multicolumn{1}{c|}{I-F1} & \multicolumn{1}{c|}{Macro F1} \\ \hline
  RITE-VAL & 0.7394 & 0.6000 & 0.5387 & 0.2083 & 0.5216 \\ \hline
  RITE-VAL-TEST & 0.6245 & 0.6135 & 0.5709 & 0.3113 & 0.5301 \\ \hline
  RITE2 & 0.6690 & 0.8378 & 0.4913 & 0.6805 & 0.6697 \\ \hline
  RITE2-TEST & 0.7734 & 0.7829 & 0.5637 & 0.6927 & 0.7032 \\ \hline
  \end{tabular}
  \caption{Results of the BERT that train with MNLI and fine-tune with RITE, using ML Features.}
  \label{result:bert_mnli_ft}
\end{table}

\section{Conclusion}
\paragraph{}
Here is the conclusion.

% \bibliography{main}
% \bibliographystyle{ieeetr}
\printbibliography

\end{CJK*}
\end{document}

% Reference 要附上期刊集數頁數等 (journal, volume, pages)
